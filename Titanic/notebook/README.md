# ğŸ““ Notebooks â€” Titanic (GUIA)

Esta pasta reÃºne os **notebooks principais do projeto Titanic**, organizados por etapa de anÃ¡lise, modelagem e submissÃ£o. Cada notebook Ã© relativamente independente, porÃ©m todos compartilham um **prÃ©-processamento padronizado**, garantindo consistÃªncia, rastreabilidade e reprodutibilidade dos resultados ao longo do projeto.

---

## ğŸ—‚ Estrutura dos Notebooks

### ğŸ” AnÃ¡lise ExploratÃ³ria de Dados (EDA)

* [**EDA.ipynb**](https://github.com/albertoakel/Kaggle/blob/master/Titanic/notebook/EDA.ipynb): AnÃ¡lise exploratÃ³ria completa do dataset Titanic, incluindo avaliaÃ§Ã£o de qualidade dos dados, distribuiÃ§Ãµes univariadas, anÃ¡lises bivariadas, dispersÃ£o, correlaÃ§Ã£o e identificaÃ§Ã£o de padrÃµes associados Ã  variÃ¡vel resposta **Survived**.
  Este notebook fundamenta diretamente as decisÃµes de **prÃ©-processamento e engenharia de atributos** utilizadas nos modelos preditivos.

---

### ğŸŒ² Modelos Ensemble â€” Random Forest

* [**models_randomForest.ipynb**](https://github.com/albertoakel/Kaggle/blob/master/Titanic/notebook/models_randomForest.ipynb):
  AvaliaÃ§Ã£o de modelos baseados em Ã¡rvores do tipo Random Forest:

  * Random Forest (baseline)
  * Random Forest com hiperparÃ¢metros ajustados

  ÃŠnfase na captura de nÃ£o linearidades, interaÃ§Ãµes entre variÃ¡veis e anÃ¡lise de robustez.

* [**Hiperparameter_search_RF.ipynb**](https://github.com/albertoakel/Kaggle/blob/master/Titanic/notebook/Hiperparameter_search_RF.ipynb):
  Busca sistemÃ¡tica de hiperparÃ¢metros para Random Forest, com validaÃ§Ã£o cruzada e anÃ¡lise comparativa de desempenho.

* **RF_Submission.ipynb**
  GeraÃ§Ã£o do arquivo de submissÃ£o Kaggle utilizando o melhor modelo Random Forest selecionado.

---

### ğŸš€ Gradient Boosting â€” XGBoost

* [**models_XGBoost.ipynb**](https://github.com/albertoakel/Kaggle/blob/master/Titanic/notebook/models_XGBoost.ipynb):
  AvaliaÃ§Ã£o de modelos XGBoost em diferentes nÃ­veis de complexidade:

  * XGBoost (baseline)
  * XGBoost com ajustes intermediÃ¡rios
  * XGBoost otimizado

  Foco em desempenho preditivo e controle de overfitting.

* [**Hiperparameter_search_XGB.ipynb**](https://github.com/albertoakel/Kaggle/blob/master/Titanic/notebook/Hiperparameter_search_XGB.ipynb):
  Busca de hiperparÃ¢metros do XGBoost, explorando regularizaÃ§Ã£o, profundidade e taxa de aprendizado.

* **XGB_Submission.ipynb**
  Notebook dedicado Ã  geraÃ§Ã£o da submissÃ£o Kaggle com o melhor modelo XGBoost.

---

### ğŸ§  Gradient Boosting â€” CatBoost

* [**models_CBTBoost.ipynb**](https://github.com/albertoakel/Kaggle/blob/master/Titanic/notebook/models_CBTBoost.ipynb):
  Modelagem utilizando CatBoost, explorando seu tratamento nativo de variÃ¡veis categÃ³ricas e estabilidade em datasets tabulares.

* [**Hiperparameter_search_CBT.ipynb**](https://github.com/albertoakel/Kaggle/blob/master/Titanic/notebook/Hiperparameter_search_CBT.ipynb):
  OtimizaÃ§Ã£o de hiperparÃ¢metros do CatBoost com validaÃ§Ã£o cruzada.

* **CBT_Submission.ipynb**
  GeraÃ§Ã£o do arquivo de submissÃ£o baseado no melhor modelo CatBoost.

---

## âš™ï¸ PadrÃµes adotados

* PrÃ©-processamento centralizado e reutilizÃ¡vel (via objetos serializados)
* Pipelines integrados (prÃ©-processamento + modelo)
* AvaliaÃ§Ã£o com mÃ©tricas adequadas Ã  classificaÃ§Ã£o binÃ¡ria (ex.: Accuracy, ROC-AUC, F1-score)
* ValidaÃ§Ã£o cruzada para anÃ¡lise de estabilidade e generalizaÃ§Ã£o

---

## âœ… Modelos finais e comparaÃ§Ãµes

Os modelos **Random Forest**, **XGBoost** e **CatBoost** sÃ£o comparados de forma consistente sob o mesmo pipeline de dados. A escolha do modelo final considera:

* Desempenho mÃ©dio em validaÃ§Ã£o cruzada
* Estabilidade entre folds
* Capacidade de generalizaÃ§Ã£o


## ğŸ“Š Resultados dos Modelos

| Modelo                            | CV ROC-AUC (Â±Ïƒ) | Test ROC-AUC | Test ACC (0.5) | Test ACC (Opt) | Status |
|-----------------------------------| --------------- | ------ |----------------| --------- | ------ |
| Random Forest (Base)              | 0.8600 Â± 0.0501 | 0.8710 | 0.8060         | 0.8134    |        |
| **Random Forest (Random Search)** | **0.8531 Â± 0.0643** | **0.8905** | **0.8246**       | **0.8358** | ğŸ†     |
| Random Forest (Refine)            | 0.8760 Â± 0.0516 | 0.8688 | 0.8060         | 0.8284    | ğŸ¥ˆ     |
| Random Forest (Bayes)             | 0.8750 Â± 0.0554 | 0.8752 | 0.8022         | 0.8172    |        |
| XGBoost (Base)                    | 0.8235 Â± 0.0485 | 0.8520 | 0.7836         | 0.8022    |        |
| XGBoost (Random Search)           | 0.8411 Â± 0.0558 | 0.8738 | 0.8172         | 0.8209    |        |
| XGBoost (Refine)                  | 0.8491 Â± 0.0494 | 0.8749 | 0.8321         | 0.8358    | ğŸ¥ˆ     |
| XGBoost (Bayes)                   | 0.8475 Â± 0.0408 | 0.8695 | 0.8134         | 0.8209    |        |
| CatBoost (Base)                   | 0.8523 Â± 0.0529 | 0.8811 | 0.8172         | 0.8284    | ğŸ¥‰     |
| CatBoost (Random Search)          | 0.8460 Â± 0.0508 | 0.8730 | 0.8134         | 0.8134    |        |
| CatBoost (Refine)                 | 0.8460 Â± 0.0466 | 0.8739 | 0.8246         | 0.8246    |        |
| CatBoost (Bayes)                  | 0.8539 Â± 0.0535 | 0.8678 | 0.8246         | 0.8284    |        |

--