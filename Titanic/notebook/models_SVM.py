#!/usr/bin/env python
# coding: utf-8

# # Duelo de Modelos

# ## 1. Bibliotecas

import time
import warnings
from pathlib import Path

import joblib
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
from scipy.stats import loguniform, randint, ttest_rel, uniform
from sklearn.base import BaseEstimator, TransformerMixin, clone
from sklearn.calibration import calibration_curve
from sklearn.compose import ColumnTransformer
from sklearn.decomposition import PCA
from sklearn.metrics import (accuracy_score, classification_report,
                             confusion_matrix, roc_auc_score,
                             average_precision_score,roc_curve)


from sklearn.model_selection import (KFold, RandomizedSearchCV,
                                     cross_val_score, cross_validate,
                                     train_test_split)
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC

# Importa√ß√µes locais 
from setup_notebook import setup_path
setup_path()
from src.model_utils import *
from src.preprocess_utils_tic import preprocessador_titanic
from src.plot_metrica_class import *

# Configura√ß√µes e Inicializa√ß√£o
warnings.filterwarnings("ignore")
print(f"\n# Processo iniciado em: {time.strftime('%H:%M:%S')}")


# ## 2. Dataload & Preprocessamento com joblib

# Dataload
BASE = Path.cwd().parent   
# =====================================================
# 0. carregamento dos preprocessador 
# =====================================================
temp = joblib.load(BASE /'src'/'preprocess_Titanic_v1.2.joblib')
PP2=temp['preprocessador']

# =====================================================
# 1. Leitura dos dados & Separa√ß√£o das bases
#  =====================================================
# As bases j√° foram previamente divididas em treino e teste antes de qualquer
# an√°lise estat√≠stica ou pr√©-processamento, evitando vazamento de informa√ß√£o
# (data leakage) durante o treinamento dos modelos. 

DATA_DIR = BASE / "data" / "raw"
X_train = pd.read_csv(DATA_DIR / "X_train_raw.csv").reset_index(drop=True)
X_test  = pd.read_csv(DATA_DIR / "X_test_raw.csv")
y_train = pd.read_csv(DATA_DIR / "y_train_raw.csv").values.ravel()
y_test  = pd.read_csv(DATA_DIR / "y_test_raw.csv")

# 
mtd_scoring='accuracy'


# ## 3. Modelos SVM

# =====================================================
# 3. Defini√ß√£o e treino dos modelo Baseline SVM 
#  =====================================================

# Os modelos SVM s√£o organizados em uma Pipeline contendo o pr√©-processamento
# (PP2) seguido da padroniza√ß√£o com StandardScaler. Esse passo, add somente para SVM, √© fundamental
# pois o m√©todo √© sens√≠vel √† escala das vari√°veis,al√©m de garantir a estabilidade do modelo. A Pipeline tamb√©m evita inconsist√™ncias
# entre treino, valida√ß√£o cruzada e teste.

modelos_para_testar = {
    "SVM (linear)": SVC(kernel='linear', random_state=42, C=1.0,probability=True),
    "SVM (Poly)": SVC(kernel='poly', random_state=42, C=1.0,probability=True),
    "SVM (RBF)": SVC(kernel='rbf', random_state=42, C=1.0,probability=True)}

print("\n#Processo iniciado em:", time.strftime("%H:%M:%S"))
print(f"\n{'='*70}")
print("Resultados Com padroniza√ß√£o StandardScaler".center(60))
print(f"{'='*70}")

pipelines = {}
resultados = {}

# Loop de execu√ß√£o
for nome, model in modelos_para_testar.items():    
    pipe = Pipeline([
        ('preprocess', PP2),
        ('scaler', StandardScaler()),
        ('model', model)])
    
    
    # Valida√ß√£o Cruzada (Baseline de Treino)
    scores = cross_val_score(pipe, X_train, y_train, cv=10, scoring=mtd_scoring, n_jobs=-1)
    
    # Treinamento e Predi√ß√£o
    pipe.fit(X_train, y_train)

    y_pred = pipe.predict(X_test)

    pipelines[nome] = pipe
    
    
    # Exibi√ß√£o dos Resultados
    print(f"{'='*70}")
    print(f"üéØ {nome} | cv_roc_auc : {scores.mean():.4f} ¬± {scores.std():.4f}")
    print(f"{'='*70}")

    best_t, score_t = best_threshold(pipe, X_test, y_test)

    print(f"üìä **Acur√°cia no Teste**: {accuracy_score(y_test, y_pred):.4f}")
    
    print(f"\nüìã **Relat√≥rio de Classifica√ß√£o**:")
    print(classification_report(y_test, y_pred))
    
    cm = confusion_matrix(y_test, y_pred)
    print(f"üéØ **Matriz de Confus√£o**:")
    print(f"                Previsto 0   Previsto 1")
    print(f"Real 0          {cm[0,0]:<11} {cm[0,1]:<11}")
    print(f"Real 1          {cm[1,0]:<11} {cm[1,1]:<11}")
    print(f"{'‚îÄ'*70}")

    resultados[nome] = {
        'scores':scores,
        'best_t':best_t,
        'score_t':score_t,
        'cv_mean': scores.mean(),
        'cv_std': scores.std(),
        'test_acc': accuracy_score(y_test, y_pred) }


# ### 3.1 Resultados Modelos SVM (Baseline)

# =====================================================
# A. Avalia√ß√£o comparativa dos modelos SVM (Baseline)
# =====================================================
# Nesta etapa √© realizada uma an√°lise comparativa completa entre os
# tr√™s kernels testados (Linear, Polynomial e RBF). O objetivo √© avaliar
# desempenho, estabilidade e diferen√ßa estat√≠stica entre os modelos.
# Calcula os scores de valida√ß√£o cruzada para cada modelo(roc-auc)


# -----------------------------------------------------
# A.1 Valida√ß√£o cruzada ROC-AUC
# -----------------------------------------------------
s1_auc = cross_val_score(pipelines['SVM (linear)'], X_train, y_train,scoring='roc_auc', cv=10)
s2_auc = cross_val_score(pipelines['SVM (Poly)'], X_train, y_train, scoring='roc_auc',cv=10)
s3_auc = cross_val_score(pipelines['SVM (RBF)'], X_train, y_train,scoring='roc_auc' ,cv=10)

# -----------------------------------------------------
# A.2 Probabilidades no conjunto de teste
# -----------------------------------------------------
y_prob1 = pipelines['SVM (linear)'].predict_proba(X_test)[:, 1]
y_prob2 = pipelines['SVM (Poly)'].predict_proba(X_test)[:, 1]
y_prob3 = pipelines['SVM (RBF)'].predict_proba(X_test)[:, 1]

# -----------------------------------------------------
# A.3 Estrutura dos dados para relatorio
# -----------------------------------------------------
models_list = [
    ('Modelo 0   (base)', pipelines['SVM (linear)'], s1_auc,resultados['SVM (linear)']['scores'], y_prob1, resultados['SVM (linear)']['best_t']),
    ('Modelo 1 (linear)', pipelines['SVM (linear)'], s1_auc,resultados['SVM (linear)']['scores'], y_prob1, resultados['SVM (linear)']['best_t']),
    ('Modelo 2  (poly) ', pipelines['SVM (Poly)'],  s2_auc,resultados['SVM (Poly)']['scores'], y_prob2,resultados['SVM (Poly)']['best_t'] ),
    ('Modelo 3  (RBF)  ', pipelines['SVM (RBF)'],  s3_auc,resultados['SVM (RBF)']['scores'], y_prob3, resultados['SVM (RBF)']['best_t'])
]

# -----------------------------------------------------
# A.4 Gera√ß√£o do relat√≥rio estat√≠stico
# -----------------------------------------------------

# ‚Ä¢ Tabela comparativa de m√©tricas (CV e Teste)
# ‚Ä¢ Compara√ß√£o estat√≠stica entre modelos
# ‚Ä¢ Teste t pareado sobre ROC-AUC
# ‚Ä¢ Identifica√ß√£o autom√°tica do melhor modelo
# ‚Ä¢ Diagn√≥stico de generaliza√ß√£o e overfitting
df_results,W = gerar_relatorio_estatistico(models_list,X_train, y_train,X_test, y_test)


# ### 3.2 Busca de hiperparametros  
# #### Modelo SVM (kernel RBF) ou tunning
#     

# =====================================================
# Busca de hiperpar√¢metros para o modelo SVM (kernel RBF)
# =====================================================
# Ap√≥s identificar o kernel RBF como melhor baseline entre os SVM testados,
# aplicamos RandomizedSearchCV para explorar o espa√ßo de hiperpar√¢metros
# de forma eficiente.

pipe_rbf = Pipeline([
    ('preprocess', PP2),
    ('scaler', StandardScaler()),
    ('model', SVC(kernel='rbf', probability=True, random_state=42))
])


# Defini√ß√£o do espa√ßo de Busca
# Utiliza distribui√ß√£o log-uniforme, apropriada para par√¢metros que
# variam em v√°rias ordens de magnitude (como C e gamma).
param_dist = {
    'model__C': loguniform(0.1, 10),         
    'model__gamma': loguniform(1e-3, 0.1),     
    'model__class_weight': [None, 'balanced']
}

n_it=100
search_rbf = RandomizedSearchCV(
    estimator=pipe_rbf,
    param_distributions=param_dist,
    n_iter=n_it,              
    scoring=mtd_scoring,  
    cv=10,
    verbose=1,
    random_state=42,
    n_jobs=-1
)

# Observa√ß√£o metodol√≥gica:
# O RandomizedSearchCV foi otimizado utilizando "accuracy".
# Posteriormente, os modelos s√£o comparados estatisticamente com base em ROC-AUC.
# Essa abordagem utiliza m√©tricas complementares: accuracy mede desempenho
# global de classifica√ß√£o, enquanto ROC-AUC avalia a capacidade do modelo
# em separar as classes independentemente do threshold de decis√£o.


# =====================================================
# Execu√ß√£o da busca de hiperpar√¢metros (Random Search)
# =====================================================
print("#Processo iniciado em:", time.strftime("%H:%M:%S"))
start = time.time()
search_rbf.fit(X_train, y_train)
end = time.time()

# Melhor pipeline encontrado
best_rbf = search_rbf.best_estimator_

y_pred_rbf=best_rbf.predict(X_test)


# =====================================================
# Otimiza√ß√£o do threshold de decis√£o
# =====================================================
# Como classificadores probabil√≠sticos usam threshold padr√£o de 0.5,
# testamos diferentes valores para verificar se existe um ponto que
# maximize a acur√°cia no conjunto de teste.
y_probs = best_rbf.predict_proba(X_test)[:, 1]
thresholds = np.linspace(0.3, 0.7, 41)

best_threshold = 0.5
max_acc = 0

for t in thresholds:
    acc = accuracy_score(y_test, y_probs > t)
    if acc > max_acc:
        max_acc = acc
        best_threshold_RBF = t

# ===============================
# Parametros encontrados
# ===============================
print(f"üìå Melhores Par√¢metros:")
print(search_rbf.best_params_)

print(f"\n{'='*70}")
print("üéØ RESULTADOS FINAIS - SVM (RBF)")
print(f"{'='*70}")

# =====================================================
# Desempenho em valida√ß√£o cruzada
# =====================================================
# A valida√ß√£o cruzada utiliza a mesma m√©trica definida
# no processo de tuning para avaliar a estabilidade do modelo.
cv_scores_rbf = cross_val_score(best_rbf, X_train, y_train,
                                cv=10,
                                scoring=mtd_scoring,
                                n_jobs=-1)

print(f"üìä CROSS-VALIDATION")
print(f"   CV Accuracy (10 folds):    {cv_scores_rbf.mean():>10.4f} ¬± {cv_scores_rbf.std():<.4f}")

# =====================================================
# Avalia√ß√£o no conjunto de teste
# =====================================================
print(f"\nüéØ TEST SET")
print(f"   Padr√£o (0.5):              {accuracy_score(y_test, y_pred_rbf):>10.4f}")
print(f"   Otimizado:                     {max_acc:.4f} (threshold={best_threshold_RBF:.3f})")
print(f"   ROC-AUC:                   {roc_auc_score(y_test, y_probs):>10.4f}")
print(f"   Avg precision:             {average_precision_score(y_test, y_probs):>10.4f}")


print(f"\nüìã **Relat√≥rio de Classifica√ß√£o**:")
print(classification_report(y_test, y_pred_rbf))

# =====================================================
# Tempo de execu√ß√£o do processo de otimiza√ß√£o
# =====================================================
print(f"\n‚è± Tempo total: {end-start:.2f} segundos")
print(f"‚è± Tempo por itera√ß√£o: {(end-start)/n_it:.2f} segundos")

print("\n#Processo finalizado em:", time.strftime("%H:%M:%S"))
print(f"{'='*70}")


# ### 3.3 Coment√°rios obre modelos SVM( baseline x tunning)
# 
# 1. **Converg√™ncia de Performance:** O fato de o `Test ACC (Opt)` ser muito pr√≥ximo (0.8209 vs 0.8172) s sugere que o SVM chegou no limite da fronteira de decis√£o. Qualquer ajuste adicional no  ou  apenas desloca os pontos na margem sem ganhar novos acertos.
# 2. **Melhoria no Limiar (Threshold):** Note que na Baseline o melhor limiar √© 0.370, enquanto no Tuning ele subiu para 0.470. Embora o modelo tunado ainda n√£o esteja perfeitamente centralizado no padr√£o 0.5, houve uma evolu√ß√£o na calibra√ß√£o: o modelo tunado est√° gerando probabilidades mais pr√≥ximas da realidade, reduzindo a necessidade de um deslocamento agressivo no threshold para otimizar a acur√°cia.
# 4. **Estabilidade (Desvio Padr√£o):** O `CV ACC` do Tuning tem um desvio padr√£o menor (0.0472 vs 0.0629). Isso √© uma vit√≥ria silenciosa: seu modelo tunado √© mais est√°vel e menos sens√≠vel a varia√ß√µes nos dados de treino.
# 
# Embora o ajuste de hiperpar√¢metros n√£o tenha superado a acur√°cia final da Baseline no teste (0.8209 vs 0.8172), o modelo tunado demonstrou maior consist√™ncia interna (menor desvio padr√£o no CV) e uma calibra√ß√£o de probabilidade superior, aproximando o limiar √≥timo (0.470) significativamente mais do padr√£o de mercado (0.5) do que a vers√£o original.

# ## 4. Submiss√£o Kaggle

# =====================================================
# Submiss√£o Kaggle
# =====================================================
# Nesta etapa utilizamos o modelo SVM-RBF otimizado para gerar
# as previs√µes no conjunto de teste oficial do Kaggle.

# Leitura da base de teste original da competi√ß√£o
base = pd.read_csv("/home/akel/PycharmProjects/Kaggle/Titanic/data/raw/test.csv")

id_test = base["PassengerId"]

x_test=base.drop(columns='PassengerId')

# =====================================================
# Gera√ß√£o das previs√µes
# =====================================================
# Utiliza o melhor modelo encontrado no processo de
# otimiza√ß√£o de hiperpar√¢metros
best_rbf = search_rbf.best_estimator_

y_predk=best_rbf.predict(x_test)

# =====================================================
# Constru√ß√£o do arquivo de submiss√£o
# =====================================================
# O Kaggle exige um CSV contendo apenas:
# PassengerId e a previs√£o da vari√°vel Survived

submission = pd.DataFrame({
    "PassengerId": id_test,
    "Survived": y_predk
})
# Caminho de salvamento do arquivo
submission_path = ("/home/akel/PycharmProjects/Kaggle/Titanic/data/processed/submission_SVM_RBF.acc_v12.csv")
# Exporta√ß√£o do arquivo
submission.to_csv(submission_path, index=False)

print("‚úÖ Arquivo de submiss√£o salvo com sucesso!")
print("\n#Processo finalizado em:", time.strftime("%H:%M:%S"))


# ## 5. Outros modelos

# =====================================================
# Carregamento de modelos previamente treinados
# =====================================================
# Nesta etapa reutilizamos modelos que foram desenvolvidos
# anteriormente no projeto principal (ver an√°lise completa
# descrita no reposit√≥rio). A estrat√©gia evita retrabalho
# computacional e permite comparar diretamente o desempenho
# do SVM com modelos j√° otimizados.

# Diret√≥rio onde os modelos est√£o armazenados
DATA_MODELS= BASE /"models"
# XGB
pipe_XGB2 = joblib.load(DATA_MODELS / 'modelo_XGB_final_refine.accuracy_v12.joblib') #pontua√ß√µes kaggle 0.7727
# randforest
pipe_RF1 = joblib.load(DATA_MODELS / 'modelo_RF_final_randsearch.roc_auc_v12.joblib') #pontua√ß√µes kaggle 0.79425


# ### 5.1 XGBoost

# ==========================================
# MODELO: XGBoost
# ==========================================
# 1 ‚Äî Desempenho em valida√ß√£o cruzada
cv_scores_xgb = cross_val_score(pipe_XGB2, X_train, y_train,
                            cv=10,
                            scoring='accuracy',
                            n_jobs=-1)


# 2 ‚Äî Treinamento do Modelo
pipe_XGB2.fit(X_train, y_train)

#3 ‚Äî Predi√ß√µes no Conjunto de Teste
y_probs = pipe_XGB2.predict_proba(X_test)[:, 1]
y_pred_xgb = pipe_XGB2.predict(X_test)

# 4 ‚Äî Otimiza√ß√£o de Threshold
thresholds = np.linspace(0.1, 0.9, 100)
accs = []
for t in thresholds:
    y_pred_t = (y_probs >= t).astype(int)
    accs.append(accuracy_score(y_test, y_pred_t))

best_idx = np.argmax(accs)
best_threshold_XGB = thresholds[best_idx]
max_acc = accs[best_idx]

print(f"{'='*70}")
print("üéØ RESULTADOS XGBOOST")
print(f"{'='*70}")
print(f"üìä CROSS-VALIDATION")
print(f"   CV Accuracy (10 folds):    {cv_scores_xgb.mean():>10.4f} ¬± {cv_scores_xgb.std():<.4f}")

# =====================================================
# Avalia√ß√£o no conjunto de teste
# =====================================================
print(f"\nüéØ TEST SET")
print(f"   Padr√£o (0.5):              {accuracy_score(y_test, y_pred_xgb):>10.4f}")
print(f"   Otimizado:                     {max_acc:.4f} (threshold={best_threshold_XGB:.3f})")
print(f"   ROC-AUC:                   {roc_auc_score(y_test, y_probs):>10.4f}")
print(f"   Avg precision:             {average_precision_score(y_test, y_probs):>10.4f}")


print(f"\nüìã **Relat√≥rio de Classifica√ß√£o**:")
print(classification_report(y_test, y_pred_xgb))


# ### 5.2 Random Forest

# ==========================================
# MODELO: Random Forest
# ==========================================
# 1 ‚Äî Desempenho em valida√ß√£o cruzada
cv_scores_rf = cross_val_score(pipe_RF1, X_train, y_train,
                            cv=10,
                            scoring='accuracy',
                            n_jobs=-1)

# 2 ‚Äî Treinamento do Modelo
pipe_RF1.fit(X_train, y_train)

#3 ‚Äî Predi√ß√µes no Conjunto de Teste
y_probs = pipe_RF1.predict_proba(X_test)[:, 1]
y_pred_rf = pipe_RF1.predict(X_test)

# 4 ‚Äî Otimiza√ß√£o de Threshold
thresholds = np.linspace(0.1, 0.9, 100)
accs = []
for t in thresholds:
    y_pred_t = (y_probs >= t).astype(int)
    accs.append(accuracy_score(y_test, y_pred_t))

best_idx = np.argmax(accs)
best_threshold_RF = thresholds[best_idx]
max_acc = accs[best_idx]

print(f"{'='*70}")
print("RESULTADOS RANDOM FOREST")
print(f"{'='*70}")
print(f"üìä CROSS-VALIDATION")
print(f"   CV Accuracy (10 folds):    {cv_scores_rf.mean():>10.4f} ¬± {cv_scores_rf.std():<.4f}")

# =====================================================
# Avalia√ß√£o no conjunto de teste
# =====================================================
print(f"\nüéØ TEST SET")
print(f"   Padr√£o (0.5):              {accuracy_score(y_test, y_pred_rf):>10.4f}")
print(f"   Otimizado:                     {max_acc:.4f} (threshold={best_threshold:.3f})")
print(f"   ROC-AUC:                   {roc_auc_score(y_test, y_probs):>10.4f}")
print(f"   Avg precision:             {average_precision_score(y_test, y_probs):>10.4f}")
print(f"\nüìã **Relat√≥rio de Classifica√ß√£o**:")
print(classification_report(y_test, y_pred_rf))


print(f"\nüéØ TEST SET")
print(f"   Padr√£o (0.5):              {accuracy_score(y_test, y_pred_rf):>10.4f}")
print(f"   Otimizado:                     {max_acc:.4f} (threshold={best_threshold:.3f})")
print(f"   ROC-AUC:                   {roc_auc_score(y_test, y_probs):>10.4f}")
print(f"   Avg precision:             {average_precision_score(y_test, y_probs):>10.4f}")


# ## 6.Resultados
# ### 6.1 Avalia√ß√£o de M√©tricas 

# =====================================================
# Avalia√ß√£o comparativa dos modelos
# =====================================================
# Nesta etapa √© realizada uma an√°lise comparativa completa entre os
# tr√™s modelos testados (SVM, XGB e Random Forest). O objetivo √© avaliar
# desempenho, estabilidade e diferen√ßa estat√≠stica entre os modelos.
# Calcula os scores de valida√ß√£o cruzada para cada modelo(roc-auc)


# -----------------------------------------------------
# Valida√ß√£o cruzada ROC-AUC
# -----------------------------------------------------
s_auc_rbf=cross_val_score(best_rbf, X_train, y_train,scoring='roc_auc', cv=10)
s_auc_xgb = cross_val_score(pipe_XGB2, X_train, y_train,scoring='roc_auc', cv=10) # XGBoost
s_auc_rf = cross_val_score(pipe_RF1, X_train, y_train, scoring='roc_auc',cv=10) # Random Forest

# -----------------------------------------------------
# Probabilidades no conjunto de teste
# -----------------------------------------------------
y_prob_rbf = best_rbf.predict_proba(X_test)[:, 1]
y_prob_xgb = pipe_XGB2.predict_proba(X_test)[:, 1]
y_prob_rf = pipe_RF1.predict_proba(X_test)[:, 1]


# -----------------------------------------------------
# 3.3 Estrutura dos dados para relatorio
# -----------------------------------------------------
models_list = [
    ('SVM (Baseline)      ', pipelines['SVM (RBF)'],  s3_auc,resultados['SVM (RBF)']['scores'], y_prob3, resultados['SVM (RBF)']['best_t']),
    ('SVM (RBF-Tunn)      ', best_rbf,s_auc_rbf,cv_scores_rbf, y_prob_rbf, best_threshold_RBF),
    ('XGBoost(vs1.2)      ', pipe_XGB2, s_auc_xgb,cv_scores_xgb,y_prob_xgb, best_threshold_XGB),
    ('Random Forest(vs1.2)', pipe_RF1,s_auc_rf,cv_scores_rf, y_prob_rf, best_threshold_RF)
]

df_results, W = gerar_relatorio_estatistico(models_list,
                                            X_train, y_train,
                                            X_test, y_test)


# ### 6.2 Figuras

# =========================
# Cria√ß√£o da figura
# Estrutura com 4 gr√°ficos (2x2) para compara√ß√£o visual dos modelos.

# =========================
# cores padronizadas
# =========================
colors = {
    "SVM": "#1f77b4",
    "XGB": "#ff7f0e",
    "RF": "#2ca02c"}

fig, axes = plt.subplots(2,2, figsize=(14,10))


# ======================================================
# A. ROC CURVE
# Avalia a capacidade discriminat√≥ria dos modelos atrav√©s
# da rela√ß√£o entre taxa de verdadeiros positivos (TPR) e
# taxa de falsos positivos (FPR).
# ======================================================
ax = axes[0,0]

fpr_rbf, tpr_rbf, _ = roc_curve(y_test, y_prob_rbf)
auc_rbf = roc_auc_score(y_test, y_prob_rbf)

fpr_xgb, tpr_xgb, _ = roc_curve(y_test, y_prob_xgb)
auc_xgb = roc_auc_score(y_test, y_prob_xgb)

fpr_rf, tpr_rf, _ = roc_curve(y_test, y_prob_rf)
auc_rf = roc_auc_score(y_test, y_prob_rf)

ax.plot(fpr_rbf, tpr_rbf, lw=2, color=colors["SVM"], label=f"SVM (RBF) AUC={auc_rbf:.3f}")
ax.plot(fpr_xgb, tpr_xgb, lw=2, color=colors["XGB"], label=f"XGBoost AUC={auc_xgb:.3f}")
ax.plot(fpr_rf, tpr_rf, lw=2, color=colors["RF"], label=f"Random Forest AUC={auc_rf:.3f}")

ax.plot([0,1],[0,1],'k--',label="Random")

ax.set_title("A: ROC Curve")
ax.set_xlabel("False Positive Rate")
ax.set_ylabel("True Positive Rate")
ax.legend()
ax.grid(alpha=0.3)


# ======================================================
# B.ROC-AUC CROSS VALIDATION
# Mostra a distribui√ß√£o dos valores de ROC-AUC obtidos nos 
# folds da valida√ß√£o cruzada para cada modelo. Permite
#avaliar estabilidade e variabilidade.
# ======================================================
ax = axes[0,1]

df_cv_auc = pd.DataFrame({
    'SVM': s_auc_rbf,
    'XGBoost': s_auc_xgb,
    'Random Forest': s_auc_rf
})

sns.boxplot(data=df_cv_auc, ax=ax, palette=[colors["SVM"],colors["XGB"],colors["RF"]])
sns.stripplot(data=df_cv_auc, ax=ax, color="black", alpha=0.5)

ax.set_title("B: ROC-AUC Distribution (10-Fold CV)")
ax.set_ylabel("ROC-AUC")
ax.grid(alpha=0.3)


# ======================================================
# C. CALIBRATION CURVE
# Avalia a calibra√ß√£o das probabilidades previstas
# pelos modelos
# ======================================================
ax = axes[1,0]

prob_true_rbf, prob_pred_rbf = calibration_curve(
    y_test.to_numpy().ravel(), y_prob_rbf, n_bins=10
)

prob_true_xgb, prob_pred_xgb = calibration_curve(
    y_test.to_numpy().ravel(), y_prob_xgb, n_bins=10
)

prob_true_rf, prob_pred_rf = calibration_curve(
    y_test.to_numpy().ravel(), y_prob_rf, n_bins=10
)

ax.plot(prob_pred_rbf, prob_true_rbf, marker='o', color=colors["SVM"], label="SVM")
ax.plot(prob_pred_xgb, prob_true_xgb, marker='s', color=colors["XGB"], label="XGBoost")
ax.plot(prob_pred_rf, prob_true_rf, marker='^', color=colors["RF"], label="Random Forest")

ax.plot([0,1],[0,1],'k--',label="Perfect")

ax.set_title("C: Calibration Curve")
ax.set_xlabel("Predicted Probability")
ax.set_ylabel("True Probability")
ax.legend()
ax.grid(alpha=0.3)


# ======================================================
# D. PROBABILITY DISTRIBUTION
# Mostra a distribui√ß√£o das probabilidades previstas
# pelos modelos. Ajuda a visualizar separa√ß√£o entre classes
# e comportamento das predi√ß√µes.
# ======================================================
ax = axes[1,1]

ax.hist(y_prob_rbf, bins=20, alpha=0.5, color=colors["SVM"], label="SVM")
ax.hist(y_prob_xgb, bins=20, alpha=0.5, color=colors["XGB"], label="XGBoost")
ax.hist(y_prob_rf, bins=20, alpha=0.5, color=colors["RF"], label="Random Forest")

ax.set_title("D: Predicted Probability Distribution")
ax.set_xlabel("Probability")
ax.set_ylabel("Frequency")
ax.legend()
ax.grid(alpha=0.3)


plt.tight_layout()
plt.show()


# ### 6.3 Resumo T√©cnico da Avalia√ß√£o dos Modelos
# 
# Neste estudo foram comparados tr√™s algoritmos de classifica√ß√£o supervisionada:
# 
# * **Support Vector Machine (SVM) com kernel RBF**
# * **XGBoost**
# * **Random Forest**
# 
# A avalia√ß√£o dos modelos foi realizada utilizando m√∫ltiplas m√©tricas de desempenho. Al√©m das m√©tricas num√©ricas, tamb√©m foram analisadas representa√ß√µes gr√°ficas importantes para avalia√ß√£o de classificadores probabil√≠sticos, como:
# 
# * **A:** Curvas ROC
# * **B:** Curvas de calibra√ß√£o
# * **C:** Distribui√ß√£o das probabilidades previstas
# * **D:** Distribui√ß√£o da ROC-AUC obtida via valida√ß√£o cruzada (10-fold)
# 
# O conjunto de dados apresenta leve desbalanceamento entre as classes, com aproximadamente 58,6% da classe 0 e 41,4% da classe 1. Em cen√°rios como este, m√©tricas baseadas em probabilidade, como ROC-AUC e Average Precision, s√£o particularmente √∫teis, uma vez que avaliam a capacidade do modelo de ordenar corretamente as observa√ß√µes entre classes independentemente de um threshold espec√≠fico.
# 
# 
# **Desempenho Geral dos Modelos**
# 
# O desempenho geral dos modelos indica que o Random Forest apresentou a melhor capacidade discriminativa, alcan√ßando ROC-AUC = 0.8905 e tamb√©m o maior valor de Average Precision = 0.8762, o que sugere maior efici√™ncia na separa√ß√£o entre as classes positivas e negativas ao longo de diferentes thresholds de decis√£o. Esses resultados indicam que o modelo consegue ordenar melhor os exemplos segundo a probabilidade de pertencimento √† classe positiva. O XGBoost apresentou desempenho intermedi√°rio para as mesmas metricas,  por√©m registrou a maior accuracy otimizada (0.8358) ap√≥s ajuste do threshold de classifica√ß√£o. J√° o SVM com kernel RBF apresentou o menor desempenho relativo, com ROC-AUC  e Average Precision. Apesar dessas diferen√ßas, observa-se que todos os modelos apresentaram desempenho competitivo, com valores de ROC-AUC superiores a 0.87, indicando boa capacidade de discrimina√ß√£o para o problema Titanic
# 
# **Estabilidade dos Modelos (Valida√ß√£o Cruzada)**
# 
# A an√°lise da estabilidade dos modelos por valida√ß√£o cruzada (10-fold) mostrou que oXGBoost apresentou a maior m√©dia de ROC-AUC durante o treinamento, com 0.8782 ¬± 0.0572, sugerindo maior consist√™ncia ao longo das diferentes parti√ß√µes da base de dados. O Random Forest apresentou m√©dia de 0.8531 ¬± 0.0643, enquanto o SVM apresentou 0.8475 ¬± 0.0615, indicando n√≠veis de variabilidade relativamente semelhantes entre os modelos. Embora o XGBoost tenha demonstrado maior estabilidade na etapa de treinamento, o Random Forest apresentou melhor desempenho no conjunto de teste, sugerindo maior capacidade de capturar padr√µes espec√≠ficos presentes nos dados de avalia√ß√£o.
# 
# **Impacto da Otimiza√ß√£o do Threshold**
# 
# Tamb√©m foi analisado o impacto da otimiza√ß√£o do threshold de classifica√ß√£o, buscando o valor que maximiza a accuracy no conjunto de teste. Os thresholds √≥timos encontrados foram 0.47 para o SVM, 0.52 para o XGBoost e 0.49 para o Random Forest. Entretanto, os ganhos obtidos com esse ajuste foram relativamente modestos, indicando que os modelos j√° operavam pr√≥ximos ao **threshold padr√£o de 0.5**, o que sugere boa calibra√ß√£o inicial das probabilidades previstas.
# 
# Os demais resultados quantitativos ‚Äî incluindo m√©tricas adicionais de desempenho, valores de valida√ß√£o cruzada e par√¢metros de decis√£o ‚Äî est√£o consolidados na **Tabela 1**, apresentada a seguir
# 
# | Modelo            | ROC-AUC    | Average Precision | Accuracy (0.5) | Accuracy (Otimizada) | Threshold √ìtimo | CV ROC-AUC (10-fold) |
# | ----------------- | ---------- | ----------------- | -------------- | -------------------- | --------------- | -------------------- |
# | **Random Forest** | **0.8905** | **0.8762**        | ~0.83          | ~0.83                | 0.49            | 0.8531 ¬± 0.0643      |
# | **XGBoost**       | 0.8749     | 0.8441            | ~0.82          | **0.8358**           | 0.52            | **0.8782 ¬± 0.0572**  |
# | **SVM (RBF)**     | 0.8711     | 0.8366            | 0.8172         | 0.8172               | 0.47            | 0.8475 ¬± 0.0615      |
# | **SVM (base)**    | 0.8769     |  -           | 0.8209         | 0.8209               | 0.370            | 0.8513 ¬± 0.0596      |
# **Tabela 1:** M√©tricas consolidadas
# 
# 
# #### A: An√°lise das Curvas ROC
# 
# As curvas ROC mostram que todos os modelos apresentam capacidade de separa√ß√£o entre as classes, refletida pelos valores elevados de AUC.
# no entanto, observa-se que a curva do Random Forest permanece ligeiramente acima das demais, especialmente em regi√µes de baixo False Positive Rate (FPR). Isso indica que o modelo consegue identificar corretamente mais casos positivos sem aumentar significativamente a taxa de falsos positivos, caracter√≠stica desej√°vel em problemas de classifica√ß√£o bin√°ria.
# 
# #### B. ROC-AUC Cross Validation
# Observa-se que o XGBoost apresentou a maior m√©dia de desempenho em valida√ß√£o cruzada (0.8782 ¬± 0.0572), indicando maior consist√™ncia entre as diferentes parti√ß√µes da base de treinamento. O Random Forest apresentou m√©dia de 0.8531 ¬± 0.0643, enquanto o SVM apresentou 0.8475 ¬± 0.0615, ambos com n√≠veis de variabilidade semelhantes. Esses resultados sugerem que, embora o Random Forest tenha apresentado melhor desempenho no conjunto de teste, o XGBoost demonstrou maior estabilidade durante o processo de treinamento.
# 
# ####  C:An√°lise da Calibration Curve
# 
# A an√°lise das curvas de calibra√ß√£o revela diferen√ßas importantes na qualidade das probabilidades previstas pelos modelos. SVM apresentou melhor alinhamento com a linha de calibra√ß√£o perfeita, sugerindo probabilidades mais bem calibradas. XGBoost e Random Forest apresentaram leve superestima√ß√£o das probabilidades, comportamento relativamente comum em modelos baseados em √°rvores. 
# 
# #### D:Distribui√ß√£o das Probabilidades Previstas
# 
# A an√°lise da distribui√ß√£o das probabilidades previstas ajuda a entender como cada modelo separa as classes. Observa-se que: o modelo Random Forest apresenta maior separa√ß√£o entre as probabilidades associadas √†s classes, como vemos nos dois picos nos extremos das probabilidades.
# .O modelo XGBoost apresenta distribui√ß√£o relativamente ampla, o que indica maior sobreposi√ß√£o entre classes; e por fim, SVM gera probabilidades mais concentradas,como o pico ~0.15%, refletindo sua natureza baseada em margens de decis√£o.
# 
# 
# ### 6.4 Pontua√ß√£o no Kaggle
# 
# O desempenho final dos modelos tamb√©m foi avaliado por meio da **submiss√£o na plataforma Kaggle**, utilizando o conjunto de teste oficial da competi√ß√£o. As pontua√ß√µes obtidas foram as seguintes:
# 
# | Modelo             | Pontua√ß√£o Kaggle |
# | ------------------ | ---------------- |
# | **Random Forest**  | **0.79425**      |
# | **XGBoost (XGB2)** | 0.77272          |
# | **SVM (RBF)**      | 0.75358          |
# 
# Observa-se que o **Random Forest apresentou o melhor desempenho no leaderboard**, corroborando os resultados obtidos nas an√°lises anteriores, especialmente em termos de **ROC-AUC e Average Precision**. O **XGBoost apresentou desempenho intermedi√°rio**, enquanto o **SVM com kernel RBF obteve a menor pontua√ß√£o entre os modelos avaliados**. Esses resultados refor√ßam a superioridade do Random Forest para este conjunto de dados e configura√ß√£o experimental.
# 
# 
# # üèÜ Considera√ß√µes gerais
# 
# Os resultados indicam que **todos os modelos avaliados apresentam desempenho s√≥lido**, com diferen√ßas relativamente pequenas entre eles.
# 
# De forma geral:
# 
# * **Random Forest apresentou o melhor desempenho global**, com maior ROC-AUC e Average Precision.
# * **XGBoost apresentou maior estabilidade em valida√ß√£o cruzada**, al√©m da melhor accuracy ap√≥s otimiza√ß√£o do threshold.
# * **SVM apresentou probabilidades mais bem calibradas**, embora com menor capacidade discriminativa.
# 
# Considerando o conjunto de m√©tricas e as an√°lises gr√°ficas, **Random Forest se mostrou a melhor escolha para este problema**, especialmente quando o objetivo √© **maximizar a separa√ß√£o entre classes e desempenho em ranking**, o que tamb√©m se reflete em seu melhor desempenho em benchmarks externos.
# 
# Nota adcional
# 

print(f"\n# Processo finalizado em: {time.strftime('%H:%M:%S')}")


# # ANEXO
# Conjunto de codigos que foram usado para dar suporte aos procedimentos.
# Detalhes completos https://github.com/albertoakel/Kaggle/tree/master/Titanic
# 

#model_utils.py
import warnings
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings('ignore', category=UserWarning,
                       message='Found unknown categories in columns')
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split, cross_val_score, KFold, RandomizedSearchCV
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score,accuracy_score
from sklearn.pipeline import Pipeline
from sklearn.base import clone



def run_model(Xo, yo, model, nome='sem nome', p=True):
    X_train, X_test, y_train, y_test = train_test_split(Xo, yo, test_size=0.3, random_state=42)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    mae = mean_absolute_error(y_test, y_pred)
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_test, y_pred)

    resultados = {
        'Modelo': 'ü§ñ ' + nome,
        'MAE': round(mae, 4),
        'RMSE': round(rmse, 4),
        'R¬≤': round(r2, 4)}

    if p != False:
        for k, v in resultados.items():
            print(f"{k}: {v}")
    return r2


# fun√ß√£o validacao_cruzada_parapipeline
def valida(Xo, yo, model, N=5, write=None):
    if write==None:
        print('Valida√ß√£o cruzada realizada!')
    kf = KFold(n_splits=N, shuffle=True, random_state=42)
    r2_scores = []

    for i, (train_idx, test_idx) in enumerate(kf.split(Xo, yo), 1):
        X_train, X_val = Xo.iloc[train_idx], Xo.iloc[test_idx]
        y_train, y_val = yo.iloc[train_idx], yo.iloc[test_idx]
        model.fit(X_train, y_train)
        y_pred = model.predict(X_val)
        r2 = r2_score(y_val, y_pred)
        r2_scores.append(r2)

    if write == 'on':
        print("=" * 44)
        print(f"valida√ß√£o cruzada (K-Fold Cross Validation)")
        print("=" * 44)

        for i, r2 in enumerate(r2_scores, 1):
            print(f"Fold {i}: R¬≤ = {r2:.4f}")

        print(f"\nüìä R¬≤ m√©dio: {np.mean(r2_scores):.4f} ¬± {np.std(r2_scores):.4f}")

    return r2_scores


def metricas_model(y_test, y_pred, nome_modelo='Modelo',write=None):
    """
    print_metricas
    """

    # Calcula as m√©tricas
    mae = mean_absolute_error(y_test, y_pred)
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_test, y_pred)

    # Cria dicion√°rio com resultados
    resultados = {
        'Modelo': nome_modelo,
        'MAE': round(mae, 4),
        'RMSE': round(rmse, 4),
        'R¬≤': round(r2, 4)
    }

    # Imprime resultados com alinhamento
    if write=='on':
        print('=' * 44)
        print(f'ü§ñ {nome_modelo.upper()}')
        print('=' * 44)
        print(f"MAE:  {resultados['MAE']:>7}")
        print(f"RMSE: {resultados['RMSE']:>6}")
        print(f"R¬≤:   {resultados['R¬≤']:>6}")

    return resultados

def pipe_models(modelo, preprocessador):
    return Pipeline([('preprocess', clone(preprocessador)),
                     ('model', modelo)])


import numpy as np



def best_threshold(model, X_test, y_test, start=0.3, stop=0.7, steps=41):
    """
    Encontra o threshold que maximiza a acur√°cia para um modelo de classifica√ß√£o.
    """
    # 1. Obt√©m as probabilidades da classe positiva
    y_probs = model.predict_proba(X_test)[:, 1]

    # 2. Define o range de busca
    thresholds = np.linspace(start, stop, steps)
    best_threshold = 0.5
    max_acc = 0

    # 3. Itera sobre os thresholds
    for t in thresholds:
        acc = accuracy_score(y_test, y_probs > t)
        if acc > max_acc:
            max_acc = acc
            best_threshold = t

    print(f"{'=' * 40}")
    print(f"üéØ Melhor Threshold: {best_threshold:.3f}")
    print(f"üìà Melhor Acur√°cia (Test): {max_acc:.4f}")
    print(f"{'=' * 40}")

    return best_threshold, max_acc


#preprocess_utils_tic.py
import os
import joblib
from datetime import datetime
import numpy as np
import pandas as pd
pd.set_option('display.max_columns', None)
# Scikit-learn - Model selection e avalia√ß√£o
from sklearn.model_selection import train_test_split
# Scikit-learn - Pr√©-processamento e pipelines
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.base import BaseEstimator, TransformerMixin


class preprocessador_titanic(BaseEstimator, TransformerMixin):

    """
    Preprocessador customizado para o dataset Titanic (Kaggle).

    Principais transforma√ß√µes
    --------------------------
    - Imputa√ß√£o de 'Embarked' pela moda
    - Cria√ß√£o de 'HasCabin' e 'Deck' a partir de 'Cabin'
    - Imputa√ß√£o hier√°rquica de 'Age' baseada em:
      (Sex, Pclass, HasCabin)
    - Cria√ß√£o de 'FamilySize'
    - Extra√ß√£o e agrupamento de t√≠tulos ('Title') do nome
    - One-hot encoding com alinhamento de colunas
    """


    def __init__(self):
        self.embarked_mode_ = None
        self.age_medians_ = {}
        self.global_age_median_ = None


    def fit(self, X, y=None):
        X = X.copy()

        if 'Embarked' in X.columns:
            # Garante que pegamos o valor (string) e n√£o a Series
            mode_series = X['Embarked'].mode()
            self.embarked_mode_ = mode_series[0] if not mode_series.empty else 'S'

        # 1. Criar HasCabin logo no in√≠cio para consist√™ncia
        if 'Cabin' in X.columns:
            X['HasCabin'] = X['Cabin'].notnull().astype(int)

        # 2. C√°lculos de m√©dia (sua l√≥gica est√° correta)
        if 'Age' in X.columns:
            self.global_age_median_ = X['Age'].median()
            group_cols = ['Sex', 'Pclass', 'HasCabin']
            for i in range(len(group_cols)):
                cols = group_cols[:len(group_cols) - i]
                self.age_medians_[tuple(cols)] = X.groupby(cols)['Age'].median()

        if 'Fare' in X.columns:
            self.fare_median_ = X['Fare'].median()  #new

        # 3. Aplicar TODAS as transforma√ß√µes que o transform faria
        # Isso garante que dummy_columns_ aprenda a estrutura final real
        X['FamilySize'] = X['SibSp'] + X['Parch'] + 1

        if 'Cabin' in X.columns:
            X['Deck'] = X['Cabin'].apply(
                lambda x: 'U' if pd.isnull(x) or str(x)[0] == 'T' else str(x)[0]
            )
            X.drop(columns='Cabin', inplace=True)

        if 'Age' in X.columns:
            X['Age2'] = X['Age']
            X.drop(columns='Age', inplace=True)

        # Novo transforma√ßao: pega pronome dos nomes (v1.2)    <-- aqui
        X['Title'] = X['Name'].str.split(', ').str[1].str.split('.', n=1).str[0]  # pegando titulos dos nomes
        X['Title'] = X['Title'].replace({"Mlle": "Miss", "Ms": "Miss"})
        X['Title'] = X['Title'].replace("Mme", "Mrs")
        common_titles = ['Mr', 'Miss', 'Mrs', 'Master']
        X['Title'] = X['Title'].apply(lambda x: x if x in common_titles else 'Rare')

        drop_cols = [c for c in ['Name', 'Ticket'] if c in X.columns]
        X.drop(columns=drop_cols, inplace=True)

        # 4. Agora sim captura as colunas do dummy
        X_dummy = pd.get_dummies(X, drop_first=False)
        self.dummy_columns_ = X_dummy.columns

        #paracatboots
        #self.final_columns_ = X.columns  # Apenas para garantir a ordem


        return self

    # =========================
    # TRANSFORM
    # =========================
    def transform(self, X):
        X = X.copy()

        # -----------------------
        # Cabin ‚Üí HasCabin + Deck
        # -----------------------
        if 'Cabin' in X.columns:
            X['HasCabin'] = X['Cabin'].notnull().astype(int)
            X['Deck'] = X['Cabin'].apply(
                lambda x: 'U' if pd.isnull(x) or str(x)[0] == 'T' else str(x)[0]
            )
            X.drop(columns='Cabin', inplace=True)

        # -----------------------
        # Embarked
        # -----------------------
        if 'Embarked' in X.columns:
            X['Embarked'] = X['Embarked'].fillna(self.embarked_mode_)

        # -----------------------
        # Age ‚Üí Age2 (hierarchical imputation)
        # -----------------------
        if 'Age' in X.columns:
            X['Age2'] = X['Age']

            for cols, medians in self.age_medians_.items():
                keys = X[list(cols)].apply(tuple, axis=1)

                X['Age2'] = X['Age2'].fillna(keys.map(medians))

            X['Age2'] = X['Age2'].fillna(self.global_age_median_)
            X.drop(columns='Age', inplace=True)

        if 'Fare' in X.columns:
            X['Fare'] = X['Fare'].fillna(self.fare_median_) #new
        # -----------------------
        # FamilySize
        # -----------------------
        X['FamilySize'] = X['SibSp'] + X['Parch'] + 1

        # Novo transforma√ßao: pega pronome dos nomes (v1.2)  <-- aqui
        X['Title'] = X['Name'].str.split(', ').str[1].str.split('.', n=1).str[0]  # pegando titulos dos nomes
        X['Title'] = X['Title'].replace({"Mlle": "Miss", "Ms": "Miss"})
        X['Title'] = X['Title'].replace("Mme", "Mrs")
        common_titles = ['Mr', 'Miss', 'Mrs', 'Master']
        X['Title'] = X['Title'].apply(lambda x: x if x in common_titles else 'Rare')

        # -----------------------
        # Drop columns
        # -----------------------
        drop_cols = [c for c in ['Name', 'Ticket'] if c in X.columns]
        X.drop(columns=drop_cols, inplace=True)

        # -----------------------
        # One-hot encoding
        # -----------------------
        X = pd.get_dummies(X, drop_first=False)
        X = X.reindex(columns=self.dummy_columns_, fill_value=0)
        #paracatboots
        #X = X.reindex(columns=self.final_columns_)  # Garante ordem das colunas

        return X

def main():
    BASE_DIR = os.path.dirname(os.path.abspath(__file__))
    ROOT_DIR = os.path.dirname(BASE_DIR)
    DATA_DIR = os.path.join(ROOT_DIR, "data","raw")

    RANDOM_STATE = 42
    TEST_SIZE = 0.3
    TARGET = 'Survived'

#   1. Leitura dos dados
    dfo = pd.read_csv("/home/akel/PycharmProjects/Kaggle/Titanic/data/raw/train.csv").drop(columns='PassengerId')

#   2. Split antes de qualquer decis√£o estat√≠stica
    X = dfo.drop(columns=TARGET)
    y = dfo[TARGET]

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE )

#
# #üß© 3. Pr√©-processadores
    PP = preprocessador_titanic()
    PP.fit(X_train)

    artifact = {'preprocessador': PP}

    artifact = {
        'preprocessador': PP,
        'metadata': {
            'dataset': 'Kaggle Titanic Survival',
            'descricao': (
                'Preprocessador customizado: Engenharia de Deck (Cabin), '
                'Cria√ß√£o de FamilySize, HasCabin, e Imputa√ß√£o Hier√°rquica de Idade '
                'baseada em Sexo, Pclass e HasCabin.'
                'agrupamento de titulos dos nomes'
            ),
            'target_transform': 'None (Binary Classification)',
            'fit_on': 'X_train only (30% test split)',
            'created_at': datetime.now().isoformat(),
            'author': 'Alberto Akel',
            'version': 'v1.2'
        }
    }

# # save files
    joblib.dump(artifact, 'preprocess_Titanic_v1.2.joblib')


    X_train.to_csv(DATA_DIR+'/X_train_raw.csv', index=False)
    X_test.to_csv(DATA_DIR+'/X_test_raw.csv', index=False)
    y_train.to_csv(DATA_DIR+'/y_train_raw.csv', index=False)
    y_test.to_csv(DATA_DIR+'/y_test_raw.csv', index=False)
    print("‚úÖ artifact e bases de treino/teste salvos com sucesso!")

if __name__ == "__main__":
    main()


#plot_metrica_class.py
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import time
from scipy.stats import ttest_rel
from sklearn.metrics import roc_curve, roc_auc_score,confusion_matrix, accuracy_score


color_palette21 = [
    "#004C4C", "#006666", "#008080", "#199191", "#29A3A3",
    "#40B5B5", "#55C7C7", "#66D9D9", "#80ECEC", "#99FFFF",
    "#FFD580", "#FFC460", "#FFB240", "#FFA020", "#FF8E00",
    "#FF7C00", "#FF6400", "#FF4C00", "#FF3300", "#FF1A00", "#FF0000"]

def gerar_relatorio_estatistico(models_list, X_train, y_train, X_test, y_test):
    """
    Gera relat√≥rio estat√≠stico completo de performance, estabilidade e signific√¢ncia
    entre m√∫ltiplos modelos bin√°rios.

    models_list: lista de tuplas
        (nome, pipeline, cv_roc_scores, cv_acc_scores, y_probs_test, best_threshold)
    """

    # ===============================
    # Fun√ß√µes auxiliares
    # ===============================
    def check_sig(p):
        return "SIM" if p < 0.05 else "N√ÉO"

    # ===============================
    # Tabela comparativa de m√©tricas
    # ===============================
    results_data = []

    for name, model, s_roc, s_acc, probs, thresh in models_list:
        test_roc = roc_auc_score(y_test, probs)
        test_acc_std = accuracy_score(y_test, probs > 0.5)
        test_acc_opt = accuracy_score(y_test, probs > thresh)

        results_data.append({
            'Modelo': name,
            'CV ROC-AUC': f"{s_roc.mean():.4f} ¬± {s_roc.std():.4f}",
            'CV ACC': f"{s_acc.mean():.4f} ¬± {s_acc.std():.4f}",
            'Test ROC-AUC': f"{test_roc:.4f}",
            'Test ACC (0.5)': f"{test_acc_std:.4f}",
            'Best Thresh': f"{thresh:.3f}",
            'Test ACC (Opt)': f"{test_acc_opt:.4f}"
        })

    df_results = pd.DataFrame(results_data)

    print(f"{'='*95}")
    print(f"{'RELAT√ìRIO DE DESEMPENHO E ESTABILIDADE ESTAT√çSTICA':^95}")
    print(f"{'='*95}")
    print(df_results.to_string(index=False, justify='center', col_space=15))

    # ===============================
    # Testes Estat√≠sticos Pareados
    # ===============================
    print(f"\n{'='*95}")
    print(f"{'AN√ÅLISE DE SIGNIFIC√ÇNCIA ESTAT√çSTICA (T-TEST PAREADO)':^95}")
    print(f"{'='*95}")

    for i in range(1, len(models_list)):
        for j in range(i + 1, len(models_list)):
            t, p = ttest_rel(models_list[i][2], models_list[j][2])
            print(f"{models_list[i][0]} vs {models_list[j][0]}: "
                  f"p-value = {p:.4f} | Diferen√ßa Significativa? {check_sig(p)}")

    # ===============================
    # Identifica√ß√£o do vencedor
    # ===============================
    df_results['ROC_Numeric'] = df_results['Test ROC-AUC'].astype(float)
    best_idx = df_results['ROC_Numeric'].idxmax()
    vencedor = df_results.iloc[best_idx]

    winner=[models_list[best_idx][0], models_list[best_idx][1]]
    baseline = df_results.iloc[0]
    ganho_roc = vencedor['ROC_Numeric'] - float(baseline['Test ROC-AUC'])

    # Signific√¢ncia vencedor vs baseline (ACC CV)
    t_stat, p_val = ttest_rel(models_list[best_idx][3], models_list[0][3])
    sig_text = (
        f"estatisticamente significativa ({p_val:.4f} < 0.05)"
        if p_val < 0.05 else
        f"n√£o significativa ({p_val:.4f} > 0.05)"
    )


    print(f"\n{'='*95}")
    print(f"{'CONCLUS√ÉO T√âCNICA AUTOM√ÅTICA':^95}")
    print(f"{'='*95}")

    print(f"1. VENCEDOR: {vencedor['Modelo']}")
    print(f"   - Ganho real sobre o Baseline: {ganho_roc:+.4f} em Test ROC-AUC.")

    print(f"\n2. ESTABILIDADE E SIGNIFIC√ÇNCIA:")
    print(f"   - A melhoria em rela√ß√£o ao Baseline √© {sig_text}.")
    print(f"   - Threshold otimizado: {vencedor['Best Thresh']}")
    print(f"   - ACC padr√£o: {vencedor['Test ACC (0.5)']}")
    print(f"   - ACC otimizada: {vencedor['Test ACC (Opt)']}")

    # ===============================
    # Overfitting / Generaliza√ß√£o
    # ===============================
    cv_roc_mean = float(vencedor['CV ROC-AUC'].split(' ¬± ')[0])
    diff_cv_test = abs(cv_roc_mean - vencedor['ROC_Numeric'])
    status_fit = "ALTA" if diff_cv_test < 0.03 else "MODERADA"

    print(f"\n3. CONFIAN√áA DO MODELO:")
    print(f"   - Ader√™ncia CV vs Teste: {status_fit} (Œî = {diff_cv_test:.4f})")

    thresh = float(vencedor['Best Thresh'])
    if thresh < 0.45:
        print("   - Estrat√©gia: modelo AGRESSIVO (threshold baixo).")
    elif thresh > 0.55:
        print("   - Estrat√©gia: modelo CONSERVADOR (threshold alto).")
    else:
        print("   - Estrat√©gia: equil√≠brio pr√≥ximo a 0.5.")

    print("\n#Processo finalizado em:", time.strftime("%H:%M:%S"))

    return df_results.sort_values(by='Test ROC-AUC', ascending=False),winner


# ##  Descritivo de Transforma√ß√µes **Preprocessador Titanic v1.2**
# 
# Este documento detalha as etapas de engenharia de atributos e limpeza de dados aplicadas ao dataset Titanic para prepar√°-lo para modelos de Machine Learning.
# 
# ### 1. Tratamento de Cabines e Decks
# 
# Para extrair valor da coluna `Cabin` (que possui muitos valores nulos), o preprocessador realiza duas opera√ß√µes:
# 
# * **HasCabin:** Cria uma vari√°vel bin√°ria (0 ou 1) indicando se o passageiro possu√≠a uma cabine registrada.
# * **Deck:** Extrai a primeira letra da cabine (ex: C, E, F), que representa o andar do navio.
# * Valores ausentes ou a cabine incomum 'T' s√£o classificados como **'U' (Unknown)**.
# * A coluna original `Cabin` √© removida.
# 
# 
# ### 2. Engenharia de T√≠tulos (Extra√ß√£o de Nome)
# 
# A partir da coluna `Name`, o preprocessador isola o t√≠tulo social do passageiro:
# 
# * **Normaliza√ß√£o:** T√≠tulos equivalentes s√£o agrupados (ex: *Mlle* e *Ms* tornam-se *Miss*; *Mme* torna-se *Mrs*).
# * **Categoriza√ß√£o:** T√≠tulos frequentes (*Mr, Miss, Mrs, Master*) s√£o mantidos, enquanto t√≠tulos nobres ou raros (como *Dr, Rev, Col, Lady*) s√£o agrupados na categoria **'Rare'**.
# * As colunas `Name` e `Ticket` s√£o descartadas ap√≥s essa extra√ß√£o.
# 
# ### 3. Imputa√ß√£o Hier√°rquica de Idade (`Age2`)
# 
# Em vez de uma m√©dia simples, o modelo utiliza uma estrat√©gia de preenchimento em n√≠veis para a coluna `Age`:
# 
# 1. **N√≠vel 1:** Tenta preencher o nulo com a mediana de passageiros com o mesmo **Sexo, Classe (Pclass) e Posse de Cabine (HasCabin)**.
# 2. **N√≠vel 2 (Fallback):** Se o grupo acima n√£o existir, utiliza combina√ß√µes simplificadas.
# 3. **N√≠vel 3 (Global):** Caso ainda reste algum valor nulo, utiliza a mediana global de idade calculada no treino.
# 
# * O resultado √© armazenado na nova coluna `Age2`.
# 
# ### 4. Din√¢mica Familiar
# 
# * **FamilySize:** Calcula o tamanho total da fam√≠lia a bordo somando `SibSp` (irm√£os/c√¥njuges), `Parch` (pais/filhos) e o pr√≥prio passageiro (+1).
# 
# ### 5. Tratamento de Dados Categ√≥ricos e Alinhamento
# 
# * **Embarked:** Preenche os valores ausentes com a **Moda** (valor mais frequente) identificada no conjunto de treino.
# * **One-Hot Encoding:** Transforma vari√°veis categ√≥ricas (`Sex`, `Embarked`, `Deck`, `Title`) em colunas bin√°rias (True/False).
# * **Reindexa√ß√£o de Colunas:** Garante que o DataFrame final tenha exatamente as mesmas colunas do momento do `fit`, preenchendo com `0` caso alguma categoria n√£o apare√ßa nos dados de teste/valida√ß√£o. Isso previne erros de dimens√£o no modelo.
# 



