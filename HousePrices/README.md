# ğŸ  House Prices â€” PrevisÃ£o de ImÃ³veis Residenciais

Projeto baseado no desafio **[House Prices â€“ Advanced Regression Techniques (Kaggle)]**.
O objetivo Ã© prever o **preÃ§o final de casas em Ames, Iowa**, utilizando tÃ©cnicas modernas de **Machine Learning para dados tabulares**.

---

## ğŸ“Œ Objetivo do Projeto

Desenvolver um pipeline completo de **prÃ©-processamento, modelagem e avaliaÃ§Ã£o**, comparando modelos lineares regularizados e mÃ©todos ensemble, com foco em:

* desempenho preditivo
* controle de overfitting
* reprodutibilidade
* organizaÃ§Ã£o para portfÃ³lio profissional

---

## ğŸ” O que vocÃª vai encontrar neste projeto

* **EDA detalhada** com anÃ¡lise estatÃ­stica e visual
*  **PrÃ©-processamento robusto** (imputaÃ§Ã£o, normalizaÃ§Ã£o, one-hot encoding)
* **Modelos avaliados**:

  * RegressÃ£o Linear
  * Ridge e LASSO
  * Random Forest Regressor
  * XGBoost
* **AvaliaÃ§Ã£o comparativa** com MAE, RMSE e RÂ²
*  **Artefatos persistidos** (preprocessador e melhor modelo)

---

## ğŸ“Š Resultados dos Modelos

AvaliaÃ§Ã£o realizada sobre o conjunto de teste (target transformado com `log1p`).

| Modelo                   | MAE        | RMSE       | RÂ²         |
| ------------------------ | ---------- | ---------- | ---------- |
| Linear Regression        | 0.0950     | 0.1826     | 0.8035     |
| Ridge (config 0)         | 0.0945     | 0.1679     | 0.8337     |
| Ridge (config 1)         | 0.0963     | 0.1346     | 0.8932     |
| LASSO (config 0)         | 0.1089     | 0.1508     | 0.8660     |
| LASSO (config 1)         | 0.0994     | 0.1384     | 0.8871     |
| Random Forest (config 0) | 0.0934     | 0.1382     | 0.8874     |
| Random Forest (config 1) | 0.0919     | 0.1383     | 0.8872     |
| XGBoost (config 0)       | 0.0976     | 0.1450     | 0.8760     |
| XGBoost (config 1)       | 0.0894     | 0.1320     | 0.8973     |
| **XGBoost (config 2)**   | **0.0838** | **0.1240** | **0.9093** |

â¡ï¸ **Melhor desempenho geral:** XGBoost (configuraÃ§Ã£o 2)

---

## ğŸ§  Principais Aprendizados

* Feature engineering e prÃ©-processamento influenciam mais que o algoritmo em si
* Modelos lineares regularizados sÃ£o fortes baselines
* XGBoost apresentou o melhor equilÃ­brio entre viÃ©s e variÃ¢ncia
* OrganizaÃ§Ã£o do pipeline Ã© essencial para evitar *data leakage*
* Persistir preprocessadores facilita inferÃªncia e deploy

---

## ğŸ“ Estrutura do Projeto

```
HousePrices/
â”‚
â”œâ”€â”€ app/                     # AplicaÃ§Ãµes futuras (deploy)
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                 # Dados originais
â”‚   â””â”€â”€ processed/           # Bases pÃ³s-processadas
â”‚
â”œâ”€â”€ image/                   # Imagens e figuras
â”‚
â”œâ”€â”€ notebook/
â”‚   â”œâ”€â”€ eda_HP.ipynb
â”‚   â”œâ”€â”€ models_Linear.ipynb
â”‚   â”œâ”€â”€ models_Random_Forest.ipynb
â”‚   â”œâ”€â”€ models_XGBoost.ipynb
â”‚   â”œâ”€â”€ XGB2_submission.ipynb
â”‚   â””â”€â”€ setup_notebook.py
â”‚
â”œâ”€â”€ sandbox/                 # Testes, rascunhos e experimentos
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ preprocess_utils.py
â”‚   â”œâ”€â”€ model_utils.py
â”‚   â”œâ”€â”€ functions.py
â”‚   â”œâ”€â”€ preprocess_house_prices_v1.joblib
â”‚   â””â”€â”€ melhor_modelo.h5
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸš€ Como executar o projeto

### 1ï¸âƒ£ Instalar dependÃªncias

```bash
pip install -r requirements.txt
```

### 2ï¸âƒ£ Executar notebooks

```bash
jupyter notebook
```

### 3ï¸âƒ£ PrÃ©-processamento automatizado

```bash
python src/preprocess_utils.py
```

---

## ğŸ“Œ ObservaÃ§Ãµes Finais

Este projeto foi estruturado com foco em **boas prÃ¡ticas de ciÃªncia de dados**, servindo tanto como **benchmark tÃ©cnico** quanto como **material de portfÃ³lio profissional**.

---
