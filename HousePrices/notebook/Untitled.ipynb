{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc5125c4-738e-448f-901b-0a08017566a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valida√ß√£o cruzada realizada!\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# üè† House Prices - XGBoost\n",
    "# =====================================================\n",
    "import pandas as pd\n",
    "\n",
    "# XGBoost\n",
    "from xgboost import XGBRegressor\n",
    "from setup_notebook import setup_path\n",
    "setup_path()\n",
    "from src.model_utils import *\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "BASE = Path.cwd().parent   \n",
    "# =====================================================\n",
    "# ‚öôÔ∏è 0. carregamento dos preprocessador \n",
    "# =====================================================\n",
    "temp = joblib.load(BASE /'src'/'preprocess_house_prices_v1.joblib')\n",
    "preprocessador=temp['preprocessador']\n",
    "colnull_train = temp['colnull_train']\n",
    "\n",
    "# =====================================================\n",
    "# üìÅ 1. Leitura dos dados & Separa√ß√£o das bases\n",
    "# =====================================================\n",
    "\n",
    "DATA_DIR = BASE / \"data\" / \"processed\"\n",
    "X_train = pd.read_csv(DATA_DIR / \"X_train_final.csv\")\n",
    "X_test  = pd.read_csv(DATA_DIR / \"X_test_final.csv\")\n",
    "y_train = pd.read_csv(DATA_DIR / \"y_train_final.csv\")\n",
    "y_test  = pd.read_csv(DATA_DIR / \"y_test_final.csv\")\n",
    "\n",
    "# =====================================================\n",
    "#  ü§ñ 4.Modelos\n",
    "# =====================================================\n",
    "model_xg0    = XGBRegressor( objective='reg:squarederror',random_state=42,n_jobs=-1 ) \n",
    "XGB0         = pipe_models(model_xg0 , preprocessador)\n",
    "\n",
    "# 1. Valida a estabilidade \n",
    "r20 = valida(X_train, y_train, model=XGB0, N=7)\n",
    "\n",
    "# 2. Testa a performance \n",
    "XGB0.fit(X_train, y_train)\n",
    "y_pred = XGB0.predict(X_test)\n",
    "res0 = metricas_model(y_test, y_pred, 'XGBoost 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da627e7d-a9b5-4e25-824d-c0d29d086a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N√∫mero de features ap√≥s pr√©-processamento: 233\n",
      "Primeiras 10 features: ['MSZoning_FV' 'MSZoning_RH' 'MSZoning_RL' 'MSZoning_RM' 'Street_Pave'\n",
      " 'LotShape_IR2' 'LotShape_IR3' 'LotShape_Reg' 'LandContour_HLS'\n",
      " 'LandContour_Low']\n",
      "\n",
      "Top 20 Features mais importantes:\n",
      "                  feature      gain\n",
      "148           OverallQual  2.969193\n",
      "169            GarageCars  1.858259\n",
      "113          CentralAir_Y  0.265090\n",
      "167            Fireplaces  0.252109\n",
      "160             GrLivArea  0.204977\n",
      "123        Functional_Sev  0.200222\n",
      "137         GarageCond_TA  0.186017\n",
      "129     GarageType_Detchd  0.169017\n",
      "92            BsmtQual_Gd  0.104711\n",
      "149           OverallCond  0.100912\n",
      "3             MSZoning_RM  0.087136\n",
      "156           TotalBsmtSF  0.080832\n",
      "153            BsmtFinSF1  0.070283\n",
      "119        KitchenQual_TA  0.066148\n",
      "150             YearBuilt  0.065197\n",
      "18   Neighborhood_ClearCr  0.059861\n",
      "125     GarageType_Attchd  0.055156\n",
      "164              HalfBath  0.050465\n",
      "86           ExterCond_TA  0.049697\n",
      "157              1stFlrSF  0.039539\n"
     ]
    }
   ],
   "source": [
    "# calcular importances\n",
    "xgboost_model = XGB0.named_steps['model']\n",
    "importances = xgboost_model.get_booster().get_score(importance_type='gain')\n",
    "\n",
    "# IMPORTANTE: Ajustar o pr√©-processador primeiro\n",
    "preprocessador.fit(X_train)  # Esta linha √© crucial!\n",
    "\n",
    "# Agora pode obter os nomes das features\n",
    "feature_names = preprocessador.get_feature_names_out()\n",
    "\n",
    "print(f\"N√∫mero de features ap√≥s pr√©-processamento: {len(feature_names)}\")\n",
    "print(f\"Primeiras 10 features: {feature_names[:10]}\")\n",
    "\n",
    "# Criar mapeamento\n",
    "feature_mapping = {f'f{i}': feature_names[i] for i in range(len(feature_names))}\n",
    "\n",
    "# Converter para DataFrame com nomes reais\n",
    "feat_imp = pd.DataFrame(list(importances.items()), columns=['feature_index', 'gain'])\n",
    "feat_imp['feature'] = feat_imp['feature_index'].map(feature_mapping)\n",
    "feat_imp = feat_imp.sort_values(by='gain', ascending=False)\n",
    "\n",
    "print(\"\\nTop 20 Features mais importantes:\")\n",
    "print(feat_imp[['feature', 'gain']].head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "896cebc5-aa26-4a86-9cbe-aa484db58a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  feature      gain\n",
      "148           OverallQual  2.969193\n",
      "169            GarageCars  1.858259\n",
      "113          CentralAir_Y  0.265090\n",
      "167            Fireplaces  0.252109\n",
      "160             GrLivArea  0.204977\n",
      "123        Functional_Sev  0.200222\n",
      "137         GarageCond_TA  0.186017\n",
      "129     GarageType_Detchd  0.169017\n",
      "92            BsmtQual_Gd  0.104711\n",
      "149           OverallCond  0.100912\n",
      "3             MSZoning_RM  0.087136\n",
      "156           TotalBsmtSF  0.080832\n",
      "153            BsmtFinSF1  0.070283\n",
      "119        KitchenQual_TA  0.066148\n",
      "150             YearBuilt  0.065197\n",
      "18   Neighborhood_ClearCr  0.059861\n",
      "125     GarageType_Attchd  0.055156\n",
      "164              HalfBath  0.050465\n",
      "86           ExterCond_TA  0.049697\n",
      "157              1stFlrSF  0.039539\n"
     ]
    }
   ],
   "source": [
    "# calcular importances\n",
    "xgboost_model = XGB0.named_steps['model']\n",
    "importances = xgboost_model.get_booster().get_score(importance_type='gain')\n",
    "\n",
    "# Obter o pr√©-processador DO PIPELINE AJUSTADO\n",
    "# Primeiro, descubra qual step √© o pr√©-processador\n",
    "for step_name, step_obj in XGB0.named_steps.items():\n",
    "    if step_name != 'model':  # O pr√©-processador √© o outro step\n",
    "        preprocessor_fitted = step_obj\n",
    "        break\n",
    "\n",
    "# Agora extrair nomes (est√° fitted porque o pipeline foi ajustado)\n",
    "feature_names = preprocessor_fitted.get_feature_names_out()\n",
    "\n",
    "# Criar mapeamento\n",
    "feature_mapping = {f'f{i}': feature_names[i] for i in range(len(feature_names))}\n",
    "\n",
    "# Converter para DataFrame com nomes reais\n",
    "feat_imp = pd.DataFrame(list(importances.items()), columns=['feature_index', 'gain'])\n",
    "feat_imp['feature'] = feat_imp['feature_index'].map(feature_mapping)\n",
    "feat_imp = feat_imp.sort_values(by='gain', ascending=False)\n",
    "\n",
    "print(feat_imp[['feature', 'gain']].head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cf4fd7-88d8-4326-ae96-af50fef81ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy import stats\n",
    "\n",
    "# def avaliar_distribuicoes(\n",
    "#     df,\n",
    "#     alpha=0.05,\n",
    "#     lim_discreto=20,\n",
    "#     max_shapiro=5000\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Avalia tipo de vari√°vel e forma da distribui√ß√£o para cada coluna do DataFrame.\n",
    "\n",
    "#     Par√¢metros\n",
    "#     ----------\n",
    "#     df : pd.DataFrame\n",
    "#     alpha : float\n",
    "#         N√≠vel de signific√¢ncia para testes de normalidade\n",
    "#     lim_discreto : int\n",
    "#         N√∫mero m√°ximo de valores √∫nicos para considerar vari√°vel discreta\n",
    "#     max_shapiro : int\n",
    "#         Tamanho m√°ximo da amostra para aplicar Shapiro-Wilk\n",
    "\n",
    "#     Retorna\n",
    "#     -------\n",
    "#     pd.DataFrame com diagn√≥stico estat√≠stico por feature\n",
    "#     \"\"\"\n",
    "\n",
    "#     resultados = []\n",
    "\n",
    "#     for col in df.columns:\n",
    "#         s = df[col].dropna()\n",
    "#         n = len(s)\n",
    "\n",
    "#         if n == 0:\n",
    "#             continue\n",
    "\n",
    "#         info = {\n",
    "#             \"feature\": col,\n",
    "#             \"n\": n,\n",
    "#             \"dtype\": str(df[col].dtype),\n",
    "#             \"n_unicos\": s.nunique()\n",
    "#         }\n",
    "\n",
    "#         # ==============================\n",
    "#         # 1Ô∏è‚É£ Tipo da vari√°vel\n",
    "#         # ==============================\n",
    "#         if pd.api.types.is_datetime64_any_dtype(s):\n",
    "#             tipo = \"data\"\n",
    "#             distribuicao = \"temporal\"\n",
    "\n",
    "#         elif pd.api.types.is_bool_dtype(s):\n",
    "#             tipo = \"bin√°ria\"\n",
    "#             distribuicao = \"bin√°ria\"\n",
    "\n",
    "#         elif pd.api.types.is_numeric_dtype(s):\n",
    "\n",
    "#             if info[\"n_unicos\"] <= 2:\n",
    "#                 tipo = \"bin√°ria\"\n",
    "#                 distribuicao = \"bin√°ria\"\n",
    "\n",
    "#             elif info[\"n_unicos\"] <= lim_discreto:\n",
    "#                 tipo = \"num√©rica discreta\"\n",
    "#             else:\n",
    "#                 tipo = \"num√©rica cont√≠nua\"\n",
    "\n",
    "#         else:\n",
    "#             tipo = \"categ√≥rica\"\n",
    "#             distribuicao = \"categ√≥rica\"\n",
    "\n",
    "#         info[\"tipo\"] = tipo\n",
    "\n",
    "#         # ==============================\n",
    "#         # 2Ô∏è‚É£ Estat√≠sticas (num√©ricas)\n",
    "#         # ==============================\n",
    "#         if pd.api.types.is_numeric_dtype(s) and tipo != \"bin√°ria\":\n",
    "\n",
    "#             mean = s.mean()\n",
    "#             median = s.median()\n",
    "#             std = s.std()\n",
    "#             skew = stats.skew(s)\n",
    "#             kurt = stats.kurtosis(s)\n",
    "\n",
    "#             info.update({\n",
    "#                 \"media\": mean,\n",
    "#                 \"mediana\": median,\n",
    "#                 \"std\": std,\n",
    "#                 \"skewness\": skew,\n",
    "#                 \"kurtosis\": kurt\n",
    "#             })\n",
    "\n",
    "#             # ==============================\n",
    "#             # 3Ô∏è‚É£ Testes de normalidade\n",
    "#             # ==============================\n",
    "#             p_shapiro = np.nan\n",
    "#             p_dagostino = np.nan\n",
    "\n",
    "#             if n <= max_shapiro:\n",
    "#                 p_shapiro = stats.shapiro(s.sample(n=min(n, max_shapiro)))[1]\n",
    "\n",
    "#             if n > 20:\n",
    "#                 p_dagostino = stats.normaltest(s)[1]\n",
    "\n",
    "#             info[\"p_shapiro\"] = p_shapiro\n",
    "#             info[\"p_dagostino\"] = p_dagostino\n",
    "\n",
    "#             normal = (\n",
    "#                 (not np.isnan(p_shapiro) and p_shapiro > alpha) or\n",
    "#                 (not np.isnan(p_dagostino) and p_dagostino > alpha)\n",
    "#             )\n",
    "\n",
    "#             # ==============================\n",
    "#             # 4Ô∏è‚É£ Classifica√ß√£o da distribui√ß√£o\n",
    "#             # ==============================\n",
    "#             if normal and abs(skew) < 0.5:\n",
    "#                 distribuicao = \"aprox. normal\"\n",
    "\n",
    "#             elif skew > 1:\n",
    "#                 distribuicao = \"assim√©trica √† direita\"\n",
    "\n",
    "#             elif skew < -1:\n",
    "#                 distribuicao = \"assim√©trica √† esquerda\"\n",
    "\n",
    "#             elif abs(skew) < 0.5 and kurt < -1:\n",
    "#                 distribuicao = \"uniforme-like\"\n",
    "\n",
    "#             elif kurt > 3:\n",
    "#                 distribuicao = \"cauda pesada\"\n",
    "\n",
    "#             else:\n",
    "#                 distribuicao = \"n√£o normal\"\n",
    "\n",
    "#         info[\"distribuicao\"] = distribuicao\n",
    "\n",
    "#         # ==============================\n",
    "#         # 5Ô∏è‚É£ Recomenda√ß√µes\n",
    "#         # ==============================\n",
    "#         if tipo == \"num√©rica cont√≠nua\":\n",
    "#             if distribuicao in [\"assim√©trica √† direita\", \"cauda pesada\"]:\n",
    "#                 rec = \"Avaliar log / Box-Cox / Yeo-Johnson\"\n",
    "#             elif distribuicao == \"aprox. normal\":\n",
    "#                 rec = \"M√©todos param√©tricos OK\"\n",
    "#             else:\n",
    "#                 rec = \"Avaliar m√©todos robustos ou n√£o param√©tricos\"\n",
    "#         elif tipo == \"categ√≥rica\":\n",
    "#             rec = \"Codifica√ß√£o (One-Hot / Ordinal)\"\n",
    "#         else:\n",
    "#             rec = \"An√°lise espec√≠fica\"\n",
    "\n",
    "#         info[\"recomendacao\"] = rec\n",
    "\n",
    "#         resultados.append(info)\n",
    "\n",
    "#     return pd.DataFrame(resultados)\n",
    "# resumo = avaliar_distribuicoes(df)\n",
    "# resumo.sort_values(\"distribuicao\").head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06707db-e15c-4919-a240-ce9fa887b712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# from scipy import stats\n",
    "# from scipy.stats import shapiro, normaltest, kstest, anderson\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# def analisar_distribuicoes(df, amostras_grandes=5000, alpha=0.05):\n",
    "#     \"\"\"\n",
    "#     Analisa as caracter√≠sticas e distribui√ß√µes das vari√°veis de um DataFrame.\n",
    "    \n",
    "#     Par√¢metros:\n",
    "#     -----------\n",
    "#     df : pandas DataFrame\n",
    "#         DataFrame a ser analisado\n",
    "#     amostras_grandes : int\n",
    "#         Limite para considerar amostras grandes (default: 5000)\n",
    "#     alpha : float\n",
    "#         N√≠vel de signific√¢ncia para testes estat√≠sticos (default: 0.05)\n",
    "    \n",
    "#     Retorna:\n",
    "#     --------\n",
    "#     dict : Dicion√°rio com an√°lises completas de cada vari√°vel\n",
    "#     \"\"\"\n",
    "    \n",
    "#     resultados = {\n",
    "#         'resumo_geral': {},\n",
    "#         'variaveis_numericas': {},\n",
    "#         'variaveis_categoricas': {},\n",
    "#         'recomendacoes': []\n",
    "#     }\n",
    "    \n",
    "#     # 1. An√°lise inicial do DataFrame\n",
    "#     resultados['resumo_geral'] = {\n",
    "#         'total_linhas': len(df),\n",
    "#         'total_colunas': len(df.columns),\n",
    "#         'tipos_dados': df.dtypes.value_counts().to_dict(),\n",
    "#         'valores_nulos_porcentagem': (df.isnull().sum() / len(df) * 100).to_dict(),\n",
    "#         'valores_unicos_por_coluna': df.nunique().to_dict()\n",
    "#     }\n",
    "    \n",
    "#     # 2. Separar vari√°veis por tipo\n",
    "#     variaveis_numericas = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "#     variaveis_categoricas = df.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "    \n",
    "#     # 3. Analisar vari√°veis num√©ricas\n",
    "#     for coluna in variaveis_numericas:\n",
    "#         dados = df[coluna].dropna()\n",
    "        \n",
    "#         # Estat√≠sticas descritivas\n",
    "#         estatisticas = {\n",
    "#             'tipo': 'num√©rica',\n",
    "#             'contagem': len(dados),\n",
    "#             'nulos': df[coluna].isnull().sum(),\n",
    "#             'media': dados.mean(),\n",
    "#             'mediana': dados.median(),\n",
    "#             'desvio_padrao': dados.std(),\n",
    "#             'variancia': dados.var(),\n",
    "#             'min': dados.min(),\n",
    "#             'max': dados.max(),\n",
    "#             'q1': dados.quantile(0.25),\n",
    "#             'q3': dados.quantile(0.75),\n",
    "#             'iqr': dados.quantile(0.75) - dados.quantile(0.25),\n",
    "#             'assimetria': dados.skew(),\n",
    "#             'curtose': dados.kurtosis(),\n",
    "#             'cv': (dados.std() / dados.mean() * 100) if dados.mean() != 0 else np.nan\n",
    "#         }\n",
    "        \n",
    "#         # Classifica√ß√£o da assimetria\n",
    "#         skew_val = estatisticas['assimetria']\n",
    "#         if abs(skew_val) < 0.5:\n",
    "#             estatisticas['classificacao_assimetria'] = 'sim√©trica'\n",
    "#         elif abs(skew_val) < 1:\n",
    "#             estatisticas['classificacao_assimetria'] = 'moderadamente assim√©trica'\n",
    "#         else:\n",
    "#             estatisticas['classificacao_assimetria'] = 'altamente assim√©trica'\n",
    "        \n",
    "#         # Testes de normalidade\n",
    "#         normalidade = {}\n",
    "        \n",
    "#         if len(dados) >= 3:  # M√≠nimo para alguns testes\n",
    "#             # Shapiro-Wilk (recomendado para n < 5000)\n",
    "#             if len(dados) < 5000:\n",
    "#                 try:\n",
    "#                     stat, p = shapiro(dados)\n",
    "#                     normalidade['shapiro_wilk'] = {\n",
    "#                         'estatistica': stat,\n",
    "#                         'p_valor': p,\n",
    "#                         'normal': p > alpha\n",
    "#                     }\n",
    "#                 except:\n",
    "#                     pass\n",
    "            \n",
    "#             # D'Agostino's K¬≤ test (funciona para amostras maiores)\n",
    "#             try:\n",
    "#                 stat, p = normaltest(dados)\n",
    "#                 normalidade['dagostino_k2'] = {\n",
    "#                     'estatistica': stat,\n",
    "#                     'p_valor': p,\n",
    "#                     'normal': p > alpha\n",
    "#                 }\n",
    "#             except:\n",
    "#                 pass\n",
    "            \n",
    "#             # Teste de Kolmogorov-Smirnov\n",
    "#             try:\n",
    "#                 # Comparar com distribui√ß√£o normal com mesma m√©dia e desvio\n",
    "#                 param = stats.norm.fit(dados)\n",
    "#                 stat, p = kstest(dados, 'norm', args=param)\n",
    "#                 normalidade['kolmogorov_smirnov'] = {\n",
    "#                     'estatistica': stat,\n",
    "#                     'p_valor': p,\n",
    "#                     'normal': p > alpha\n",
    "#                 }\n",
    "#             except:\n",
    "#                 pass\n",
    "            \n",
    "#             # Teste de Anderson-Darling\n",
    "#             try:\n",
    "#                 resultado = anderson(dados, dist='norm')\n",
    "#                 normalidade['anderson_darling'] = {\n",
    "#                     'estatistica': resultado.statistic,\n",
    "#                     'valores_criticos': resultado.critical_values.tolist(),\n",
    "#                     'niveis_significancia': resultado.significance_level.tolist()\n",
    "#                 }\n",
    "#             except:\n",
    "#                 pass\n",
    "        \n",
    "#         estatisticas['testes_normalidade'] = normalidade\n",
    "        \n",
    "#         # Tentativa de identificar distribui√ß√£o\n",
    "#         distribuicao_identificada = identificar_distribuicao(dados, alpha)\n",
    "#         estatisticas['distribuicao_identificada'] = distribuicao_identificada\n",
    "        \n",
    "#         # Identifica√ß√£o de outliers usando IQR\n",
    "#         Q1 = estatisticas['q1']\n",
    "#         Q3 = estatisticas['q3']\n",
    "#         IQR = estatisticas['iqr']\n",
    "#         limite_inferior = Q1 - 1.5 * IQR\n",
    "#         limite_superior = Q3 + 1.5 * IQR\n",
    "#         outliers = dados[(dados < limite_inferior) | (dados > limite_superior)]\n",
    "        \n",
    "#         estatisticas['outliers_iqr'] = {\n",
    "#             'limite_inferior': limite_inferior,\n",
    "#             'limite_superior': limite_superior,\n",
    "#             'quantidade': len(outliers),\n",
    "#             'porcentagem': (len(outliers) / len(dados)) * 100 if len(dados) > 0 else 0\n",
    "#         }\n",
    "        \n",
    "#         resultados['variaveis_numericas'][coluna] = estatisticas\n",
    "    \n",
    "#     # 4. Analisar vari√°veis categ√≥ricas\n",
    "#     for coluna in variaveis_categoricas:\n",
    "#         dados = df[coluna].dropna()\n",
    "        \n",
    "#         estatisticas = {\n",
    "#             'tipo': str(df[coluna].dtype),\n",
    "#             'contagem': len(dados),\n",
    "#             'nulos': df[coluna].isnull().sum(),\n",
    "#             'valores_unicos': dados.nunique(),\n",
    "#             'moda': dados.mode().iloc[0] if not dados.empty else None,\n",
    "#             'frequencia_moda': dados.value_counts().iloc[0] if not dados.empty else 0,\n",
    "#             'entropia': calcular_entropia(dados),\n",
    "#             'distribuicao_categorias': dados.value_counts(normalize=True).to_dict()\n",
    "#         }\n",
    "        \n",
    "#         # Classifica√ß√£o do tipo de vari√°vel categ√≥rica\n",
    "#         if dados.nunique() == 2:\n",
    "#             estatisticas['tipo_categorica'] = 'bin√°ria'\n",
    "#         elif dados.nunique() <= 10:\n",
    "#             estatisticas['tipo_categorica'] = 'nominal_pequena'\n",
    "#         elif dados.nunique() <= 50:\n",
    "#             estatisticas['tipo_categorica'] = 'nominal_grande'\n",
    "#         else:\n",
    "#             estatisticas['tipo_categorica'] = 'textual/alta_cardinalidade'\n",
    "        \n",
    "#         resultados['variaveis_categoricas'][coluna] = estatisticas\n",
    "    \n",
    "#     # 5. Gerar recomenda√ß√µes\n",
    "#     gerar_recomendacoes(resultados)\n",
    "    \n",
    "#     return resultados\n",
    "\n",
    "\n",
    "# def identificar_distribuicao(dados, alpha=0.05):\n",
    "#     \"\"\"\n",
    "#     Tenta identificar a distribui√ß√£o dos dados.\n",
    "#     \"\"\"\n",
    "#     if len(dados) < 30:\n",
    "#         return {'distribuicao': 'amostra_pequena', 'confianca': 'baixa'}\n",
    "    \n",
    "#     resultados = []\n",
    "    \n",
    "#     # Testar distribui√ß√£o normal\n",
    "#     try:\n",
    "#         _, p_normal = normaltest(dados)\n",
    "#         resultados.append(('normal', p_normal))\n",
    "#     except:\n",
    "#         pass\n",
    "    \n",
    "#     # Testar distribui√ß√£o log-normal\n",
    "#     try:\n",
    "#         dados_positivos = dados[dados > 0]\n",
    "#         if len(dados_positivos) > 30:\n",
    "#             dados_log = np.log(dados_positivos)\n",
    "#             _, p_lognormal = normaltest(dados_log)\n",
    "#             resultados.append(('log_normal', p_lognormal))\n",
    "#     except:\n",
    "#         pass\n",
    "    \n",
    "#     # Testar distribui√ß√£o exponencial\n",
    "#     try:\n",
    "#         param = stats.expon.fit(dados)\n",
    "#         stat, p_expon = kstest(dados, 'expon', args=param)\n",
    "#         resultados.append(('exponencial', p_expon))\n",
    "#     except:\n",
    "#         pass\n",
    "    \n",
    "#     # Verificar simetria e curtose para infer√™ncia\n",
    "#     skewness = stats.skew(dados)\n",
    "#     kurt = stats.kurtosis(dados)\n",
    "    \n",
    "#     # Classifica√ß√£o baseada em estat√≠sticas descritivas\n",
    "#     if abs(skewness) < 0.5 and abs(kurt) < 0.5:\n",
    "#         inferencia = 'sim√©trica_mesoc√∫rtica (similar √† normal)'\n",
    "#     elif skewness > 1:\n",
    "#         inferencia = 'assim√©trica_positiva'\n",
    "#     elif skewness < -1:\n",
    "#         inferencia = 'assim√©trica_negativa'\n",
    "#     elif kurt > 1:\n",
    "#         inferencia = 'leptoc√∫rtica (picos agudos)'\n",
    "#     elif kurt < -1:\n",
    "#         inferencia = 'platic√∫rtica (achatada)'\n",
    "#     else:\n",
    "#         inferencia = 'distribui√ß√£o_regular'\n",
    "    \n",
    "#     # Escolher a melhor distribui√ß√£o baseada nos testes\n",
    "#     if resultados:\n",
    "#         melhor_dist = max(resultados, key=lambda x: x[1])\n",
    "#         distribuicao = melhor_dist[0] if melhor_dist[1] > alpha else 'desconhecida'\n",
    "#         confianca = 'alta' if melhor_dist[1] > 0.1 else 'moderada'\n",
    "#     else:\n",
    "#         distribuicao = 'desconhecida'\n",
    "#         confianca = 'baixa'\n",
    "    \n",
    "#     return {\n",
    "#         'distribuicao_teste': distribuicao,\n",
    "#         'inferencia_descritiva': inferencia,\n",
    "#         'assimetria': skewness,\n",
    "#         'curtose': kurt,\n",
    "#         'confianca': confianca,\n",
    "#         'resultados_testes': resultados\n",
    "#     }\n",
    "\n",
    "\n",
    "# def calcular_entropia(dados):\n",
    "#     \"\"\"\n",
    "#     Calcula a entropia de Shannon para vari√°veis categ√≥ricas.\n",
    "#     \"\"\"\n",
    "#     if len(dados) == 0:\n",
    "#         return 0\n",
    "    \n",
    "#     contagens = dados.value_counts()\n",
    "#     proporcoes = contagens / len(dados)\n",
    "#     entropia = -np.sum(proporcoes * np.log2(proporcoes))\n",
    "    \n",
    "#     return entropia\n",
    "\n",
    "\n",
    "# def gerar_recomendacoes(resultados):\n",
    "#     \"\"\"\n",
    "#     Gera recomenda√ß√µes baseadas na an√°lise.\n",
    "#     \"\"\"\n",
    "#     recomendacoes = []\n",
    "    \n",
    "#     # Analisar vari√°veis num√©ricas\n",
    "#     for coluna, info in resultados['variaveis_numericas'].items():\n",
    "#         # Verificar normalidade\n",
    "#         normal = any(teste.get('normal', False) \n",
    "#                     for teste in info.get('testes_normalidade', {}).values())\n",
    "        \n",
    "#         if not normal:\n",
    "#             recomendacoes.append(\n",
    "#                 f\"Vari√°vel '{coluna}': N√£o normal - considerar transforma√ß√µes (log, box-cox) \"\n",
    "#                 f\"ou usar testes n√£o param√©tricos\"\n",
    "#             )\n",
    "        \n",
    "#         # Verificar outliers\n",
    "#         outliers_pct = info['outliers_iqr']['porcentagem']\n",
    "#         if outliers_pct > 5:\n",
    "#             recomendacoes.append(\n",
    "#                 f\"Vari√°vel '{coluna}': {outliers_pct:.1f}% outliers detectados - \"\n",
    "#                 f\"verificar se s√£o erros ou dados v√°lidos\"\n",
    "#             )\n",
    "        \n",
    "#         # Verificar assimetria\n",
    "#         if abs(info['assimetria']) > 1:\n",
    "#             recomendacoes.append(\n",
    "#                 f\"Vari√°vel '{coluna}': Altamente assim√©trica (skew={info['assimetria']:.2f}) - \"\n",
    "#                 f\"considerar transforma√ß√µes\"\n",
    "#             )\n",
    "    \n",
    "#     # Analisar vari√°veis categ√≥ricas\n",
    "#     for coluna, info in resultados['variaveis_categoricas'].items():\n",
    "#         if info['valores_unicos'] > 50:\n",
    "#             recomendacoes.append(\n",
    "#                 f\"Vari√°vel '{coluna}': Alta cardinalidade ({info['valores_unicos']} valores √∫nicos) - \"\n",
    "#                 f\"considerar agrupamento ou t√©cnicas espec√≠ficas\"\n",
    "#             )\n",
    "    \n",
    "#     # Verificar dados faltantes\n",
    "#     for coluna, pct in resultados['resumo_geral']['valores_nulos_porcentagem'].items():\n",
    "#         if pct > 20:\n",
    "#             recomendacoes.append(\n",
    "#                 f\"Vari√°vel '{coluna}': {pct:.1f}% dados faltantes - \"\n",
    "#                 f\"avaliar impacto e estrat√©gia de imputa√ß√£o\"\n",
    "#             )\n",
    "    \n",
    "#     resultados['recomendacoes'] = recomendacoes\n",
    "\n",
    "\n",
    "# def gerar_relatorio(resultados, formato='texto'):\n",
    "#     \"\"\"\n",
    "#     Gera um relat√≥rio formatado da an√°lise.\n",
    "    \n",
    "#     Par√¢metros:\n",
    "#     -----------\n",
    "#     resultados : dict\n",
    "#         Resultados da fun√ß√£o analisar_distribuicoes\n",
    "#     formato : str\n",
    "#         'texto' ou 'dataframe' para formato de sa√≠da\n",
    "#     \"\"\"\n",
    "#     if formato == 'texto':\n",
    "#         # Resumo geral\n",
    "#         print(\"=\" * 80)\n",
    "#         print(\"AN√ÅLISE DE DISTRIBUI√á√ïES - RELAT√ìRIO\")\n",
    "#         print(\"=\" * 80)\n",
    "        \n",
    "#         resumo = resultados['resumo_geral']\n",
    "#         print(f\"\\n1. RESUMO GERAL:\")\n",
    "#         print(f\"   Total de observa√ß√µes: {resumo['total_linhas']}\")\n",
    "#         print(f\"   Total de vari√°veis: {resumo['total_colunas']}\")\n",
    "#         print(f\"   Tipos de dados: {resumo['tipos_dados']}\")\n",
    "        \n",
    "#         print(f\"\\n2. VARI√ÅVEIS NUM√âRICAS:\")\n",
    "#         for coluna, info in resultados['variaveis_numericas'].items():\n",
    "#             print(f\"\\n   {coluna}:\")\n",
    "#             print(f\"   - Tipo: {info['tipo']}\")\n",
    "#             print(f\"   - Distribui√ß√£o: {info['distribuicao_identificada']['distribuicao_teste']}\")\n",
    "#             print(f\"   - Assimetria: {info['assimetria']:.3f} ({info['classificacao_assimetria']})\")\n",
    "#             print(f\"   - Outliers: {info['outliers_iqr']['quantidade']} \"\n",
    "#                   f\"({info['outliers_iqr']['porcentagem']:.1f}%)\")\n",
    "        \n",
    "#         print(f\"\\n3. VARI√ÅVEIS CATEG√ìRICAS:\")\n",
    "#         for coluna, info in resultados['variaveis_categoricas'].items():\n",
    "#             print(f\"\\n   {coluna}:\")\n",
    "#             print(f\"   - Tipo: {info['tipo_categorica']}\")\n",
    "#             print(f\"   - Valores √∫nicos: {info['valores_unicos']}\")\n",
    "#             print(f\"   - Entropia: {info['entropia']:.3f}\")\n",
    "        \n",
    "#         print(f\"\\n4. RECOMENDA√á√ïES:\")\n",
    "#         for i, rec in enumerate(resultados['recomendacoes'], 1):\n",
    "#             print(f\"   {i}. {rec}\")\n",
    "        \n",
    "#         print(\"\\n\" + \"=\" * 80)\n",
    "#         print(\"FIM DO RELAT√ìRIO\")\n",
    "#         print(\"=\" * 80)\n",
    "    \n",
    "#     elif formato == 'dataframe':\n",
    "#         # Criar DataFrames resumidos\n",
    "#         df_numericas = pd.DataFrame.from_dict(\n",
    "#             resultados['variaveis_numericas'], \n",
    "#             orient='index'\n",
    "#         )\n",
    "        \n",
    "#         df_categoricas = pd.DataFrame.from_dict(\n",
    "#             resultados['variaveis_categoricas'], \n",
    "#             orient='index'\n",
    "#         )\n",
    "        \n",
    "#         return {\n",
    "#             'numericas': df_numericas,\n",
    "#             'categoricas': df_categoricas,\n",
    "#             'recomendacoes': resultados['recomendacoes']\n",
    "#         }\n",
    "\n",
    "\n",
    "# # Fun√ß√£o auxiliar para visualiza√ß√£o\n",
    "# def plotar_distribuicoes(df, variaveis=None, figsize=(15, 10)):\n",
    "#     \"\"\"\n",
    "#     Plota distribui√ß√µes das vari√°veis.\n",
    "#     \"\"\"\n",
    "#     if variaveis is None:\n",
    "#         variaveis = df.columns\n",
    "    \n",
    "#     n_variaveis = len(variaveis)\n",
    "#     n_cols = 3\n",
    "#     n_rows = (n_variaveis + n_cols - 1) // n_cols\n",
    "    \n",
    "#     fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)\n",
    "#     axes = axes.flatten()\n",
    "    \n",
    "#     for idx, coluna in enumerate(variaveis):\n",
    "#         ax = axes[idx]\n",
    "#         dados = df[coluna].dropna()\n",
    "        \n",
    "#         if pd.api.types.is_numeric_dtype(df[coluna]):\n",
    "#             # Plot para vari√°veis num√©ricas\n",
    "#             ax.hist(dados, bins='auto', alpha=0.7, edgecolor='black')\n",
    "#             ax.axvline(dados.mean(), color='red', linestyle='--', label=f'M√©dia: {dados.mean():.2f}')\n",
    "#             ax.axvline(dados.median(), color='green', linestyle='--', \n",
    "#                       label=f'Mediana: {dados.median():.2f}')\n",
    "#             ax.set_title(f'{coluna}\\n(skew={dados.skew():.2f}, kurt={dados.kurtosis():.2f})')\n",
    "#             ax.legend()\n",
    "#             ax.set_xlabel('Valor')\n",
    "#             ax.set_ylabel('Frequ√™ncia')\n",
    "#         else:\n",
    "#             # Plot para vari√°veis categ√≥ricas\n",
    "#             top_categorias = dados.value_counts().head(10)\n",
    "#             ax.bar(top_categorias.index.astype(str), top_categorias.values)\n",
    "#             ax.set_title(f'{coluna}\\n({len(dados.unique())} categorias)')\n",
    "#             ax.set_xlabel('Categoria')\n",
    "#             ax.set_ylabel('Contagem')\n",
    "#             plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "#     # Ocultar eixos vazios\n",
    "#     for idx in range(len(variaveis), len(axes)):\n",
    "#         axes[idx].set_visible(False)\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "# # Exemplo de uso\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Criar DataFrame de exemplo\n",
    "   \n",
    "\n",
    "#     # Executar an√°lise\n",
    "#     resultados = analisar_distribuicoes(df)\n",
    "    \n",
    "#     # Gerar relat√≥rio\n",
    "#     gerar_relatorio(resultados, formato='texto')\n",
    "    \n",
    "#     # Visualizar distribui√ß√µes\n",
    "#     plotar_distribuicoes(df)\n",
    "    \n",
    "#     # Obter resultados em DataFrames\n",
    "#     dfs_resultados = gerar_relatorio(resultados, formato='dataframe')\n",
    "#     print(\"\\nResumo num√©rico:\")\n",
    "#     print(dfs_resultados['numericas'][['media', 'assimetria', 'outliers_iqr']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d6895e-c55f-455c-86a9-e061e4652459",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# #columns\n",
    "# # Index(['MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'Street',\n",
    "# #        'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope',\n",
    "# #        'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle',\n",
    "# #        'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'RoofStyle',\n",
    "# #        'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrArea', 'ExterQual',\n",
    "# #        'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure',\n",
    "# #        'BsmtFinType1', 'BsmtFinSF1', 'BsmtFinType2', 'BsmtFinSF2', 'BsmtUnfSF',\n",
    "# #        'TotalBsmtSF', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical',\n",
    "# #        '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath',\n",
    "# #        'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr',\n",
    "# #        'KitchenQual', 'TotRmsAbvGrd', 'Functional', 'Fireplaces', 'GarageType',\n",
    "# #        'GarageYrBlt', 'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQual',\n",
    "# #        'GarageCond', 'PavedDrive', 'WoodDeckSF', 'OpenPorchSF',\n",
    "# #        'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal',\n",
    "# #        'MoSold', 'YrSold', 'SaleType', 'SaleCondition', 'SalePrice'],\n",
    "# #       dtype='object')\n",
    "\n",
    "# import plotly.graph_objects as go\n",
    "# import scipy.stats as st\n",
    "# import numpy as np\n",
    "\n",
    "# # 1. Preparar os dados\n",
    "# y = df['GarageArea'] # <-- caixa de sele√ß√£o\n",
    "\n",
    "# # 2. C√°lculos Estat√≠sticos \n",
    "# params_johnson = st.johnsonsu.fit(y)\n",
    "# params_norm = st.norm.fit(y)\n",
    "\n",
    "# x_range = np.linspace(y.min(), y.max(), 200)\n",
    "# pdf_johnson = st.johnsonsu.pdf(x_range, *params_johnson)\n",
    "# pdf_norm = st.norm.pdf(x_range, *params_norm)\n",
    "\n",
    "# # 3. Constru√ß√£o do Gr√°fico\n",
    "# fig = go.Figure()\n",
    "\n",
    "# # Histograma usando a primeira cor da sua paleta\n",
    "# fig.add_trace(go.Histogram(\n",
    "#     x=y,\n",
    "#     histnorm='probability density',\n",
    "#     name='Distribui√ß√£o Real',\n",
    "#     marker_color=color_palette21[0], \n",
    "#     opacity=0.6,\n",
    "#     marker=dict(line=dict(width=1, color='white')) )\n",
    "# )\n",
    "\n",
    "# # Linha de Ajuste Johnson SU (usando um tom de laranja da sua paleta para destaque)\n",
    "# fig.add_trace(go.Scatter(\n",
    "#     x=x_range, y=pdf_johnson,\n",
    "#     mode='lines',\n",
    "#     name='Ajuste Johnson SU',\n",
    "#     line=dict(color=color_palette21[18], width=4) # Laranja forte\n",
    "# ))\n",
    "\n",
    "# # Linha de Ajuste Normal (usando um tom mais claro para compara√ß√£o)\n",
    "# fig.add_trace(go.Scatter(\n",
    "#     x=x_range, y=pdf_norm,\n",
    "#     mode='lines',\n",
    "#     name='Ajuste Normal (Gaussiana)',\n",
    "#     line=dict(color=color_palette21[10], width=3, dash='dash')\n",
    "# ))\n",
    "\n",
    "# # 4. Layout\n",
    "# fig.update_layout(\n",
    "#     title='An√°lise de Distribui√ß√£o: SalePrice',\n",
    "#     xaxis_title='SalePrice',\n",
    "#     yaxis_title='Densidade',\n",
    "#     template='plotly_white',\n",
    "#     legend=dict(yanchor=\"top\", y=0.99, xanchor=\"right\", x=0.99),\n",
    "#     width=1000,\n",
    "#     height=720\n",
    "# )\n",
    "\n",
    "# fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
