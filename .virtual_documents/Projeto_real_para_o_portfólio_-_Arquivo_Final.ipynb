


# Importando o pandas
import pandas as pd


# Importando o dataset de treino
base = pd.read_csv('train.csv')


# Visualizando essa base
base.head(3)


# Retornando o shape da base
base.shape


# E as informações
#base.info()





# Visualizando quantidade de valores vazios
(base.isnull().sum()/base.shape[0]).sort_values(ascending=False).head(20)


# Podemos eliminar as colunas com mais de 10% de valores vazios
eliminar = base.columns[(base.isnull().sum()/base.shape[0]) > 0.1]
eliminar


# Eliminando essas colunas
base = base.drop(eliminar,axis=1)





# Selecionando apenas as colunas numéricas
colunas = base.columns[base.dtypes != 'object']
colunas


# E criar uma nova base com esses valores
base2 = base.loc[:,colunas]
base2.head(3)


# Verificando os valores vazios
base2.isnull().sum().sort_values(ascending=False).head(3)


# Substituindo os valores vazios por -1
base2 = base2.fillna(-1)











# Selecionando X e y
X = base2.drop('SalePrice',axis=1)
y = base2.SalePrice


# Importando o train_test_split
from sklearn.model_selection import train_test_split


# Separando essa base em treino e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)





# Importando a regressão linear
from sklearn.linear_model import LinearRegression


# Criando o regressor e fazendo o fit com os dados de treino
reg_rl = LinearRegression().fit(X_train, y_train)


# Fazendo a previsão pros dados de teste
y_rl = reg_rl.predict(X_test)


# Importando a árvore de regressão
from sklearn import tree


# Criando o regressor e fazendo o fit com os dados de treino
reg_ar = tree.DecisionTreeRegressor(random_state=42).fit(X_train, y_train)


# Fazendo a previsão
y_ar = reg_ar.predict(X_test)


# Importando o KNN
from sklearn.neighbors import KNeighborsRegressor


# Criando o regressor e fazendo o fit com os dados de treino
reg_knn = KNeighborsRegressor(n_neighbors=2).fit(X_train, y_train)


# Fazendo a previsão
y_knn = reg_knn.predict(X_test)





# Importando o erro médio absoluto
from sklearn.metrics import mean_absolute_error


# E o erro quadrático médio
from sklearn.metrics import mean_squared_error


# Avaliando o erro da regressão
print(mean_absolute_error(y_test,y_rl))
print(mean_squared_error(y_test,y_rl))


# da árvore de decisão
print(mean_absolute_error(y_test,y_ar))
print(mean_squared_error(y_test,y_ar))


# e do knn
print(mean_absolute_error(y_test,y_knn))
print(mean_squared_error(y_test,y_knn))





# Importando o matplotlib
import matplotlib.pyplot as plt


# Criando esse gráfico
fig, ax = plt.subplots(ncols=3,figsize=(15,5))

ax[0].scatter(y_test/100000,y_rl/100000)
ax[0].plot([0,700000],[0,700000],'--r')
ax[1].scatter(y_test/100000,y_ar/100000)
ax[1].plot([0,700000],[0,700000],'--r')
ax[2].scatter(y_test/100000,y_knn/100000)
ax[2].plot([0,700000],[0,700000],'--r')

ax[0].set(xlim=(0, 7),ylim=(0, 7))
ax[0].set_xlabel('Real')
ax[0].set_ylabel('Previsão')
ax[1].set(xlim=(0, 7),ylim=(0, 7))
ax[1].set_xlabel('Real')
ax[1].set_ylabel('Previsão')
ax[2].set(xlim=(0, 7),ylim=(0, 7))
ax[2].set_xlabel('Real')
ax[2].set_ylabel('Previsão')

plt.show()








# Importando a base de teste
teste = pd.read_csv('test.csv')


# Visualizando a base
teste.head(3)





# Eliminando as mesmas colunas da base de treino
teste = teste.drop(eliminar,axis=1)


# Verificando as colunas numéricas
colunas2 = teste.columns[teste.dtypes != 'object']
colunas2


# Mantendo também apenas as colunas numéricas
teste = teste.loc[:,colunas2]


# Verificando a base restante
teste.info()


# Visualizando quantidade de valores vazios
teste.isnull().sum().sort_values(ascending=False).head(10)





# Substituindo os valores vazios por -1
teste = teste.fillna(-1)





# Vamos usar a regressão linear para fazer a previsão
y_pred = reg_rl.predict(teste)


# Podemos adicionar essa coluna de previsão na nossa base
teste['SalePrice'] = y_pred


# E extrair somente o Id e o SalePrice
resultado = teste[['Id','SalePrice']]
resultado.head(3)


# Podemos então exportar essa base
resultado.to_csv('resultado.csv',index=False)



